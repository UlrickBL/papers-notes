Papier / Concept,Notes en Français (Originales),Traduction en Anglais
Diffusion et token meta,l'image est faite complètement lors de l'attention avec patch de vae et diffusion unet et le modèle switch entre text et Diffusion à partir de tokens pour la génération ,"the image is generated completely during attention with VAE patch and Diffusion UNet, and the model switches between text and Diffusion based on tokens for generation"
Diffusion avec attention et latent,"c'est de la cross attention avec le texte, un latent vae pour les perfs et un unet avec du transformer pour prédire le noise ","it's cross-attention with text, a latent VAE for performance, and a UNet with a transformer to predict the noise"
Xformers,pas forcément fou à lire ,not necessarily essential reading
Mlir pour les compilers,lire le diapo intro de Google ,read the Google intro slide deck
Autogen et autogen studio,plusieurs interactions d'agents pour augmenter les performances : à tester avec l'ui [cite: 261],multiple agent interactions to increase performance: to test with the UI [cite: 261]
Text2sql is not ennough,en gros utilise le LLM dans la query sous forme d'agent [cite: 261],basically use the LLM in the query as an agent
ReMamba,double forward pour sélectionner les latents intéressants pour la query [cite: 261],double forward to select interesting latents for the query
Spéculative decoding,"on utilise un petit modèle de façon itérative pour c tokens puis on prédit de façon batchée les x séquences, si le même mot est prédit par le gros c'est bon [cite: 261]","we use a small model iteratively for c tokens, then we predict the x sequences in a batched manner; if the same word is predicted by the large model, it's good"
Whisper,concept compris mais pourquoi pas lire le papier [cite: 261],concept understood but why not read the paper
Agentic rag for time series analysis,pas très intéressant [cite: 261],not very interesting
Kosmos,mais que anglais [cite: 261],but English only
Training de llama 3.1,par le mec de LinkedIn [cite: 262],by the guy from LinkedIn
Quiet star,on raisonne lorsqu'il y a des tokens thought et on sample dans les logits pour pas faire trop de compute [cite: 262],we reason when there are thought tokens and we sample in the logits to avoid too much computation
Lets verify step by step,c'est mieux de juger une réponse avec un process plutôt que outcome pour un générateur. [cite: 262],it's better to judge a response with a process rather than an outcome for a generator. [cite: 262]
Active learning,performe mieux pour le reward model sur les cas difficiles [cite: 263],performs better for the reward model on difficult cases
Tree of thought,,
Agent q,"prompt react et thought, dpo sur le prompt et agent avec vision qui critique le choix des actions + tree search pour améliorer les perfs et itérations [cite: 263]","prompt react and thought, DPO on the prompt and agent with vision that critiques the choice of actions + tree search to improve performance and iterations"
Yarn (RoPE),rope est en fait sur q et k donc que au niveau du softmax de l'attention pas la value [cite: 263],"RoPE is actually on q and k, so only at the attention softmax level, not the value"
Position interpolation,on multiplie la position m par L/L′[cite: 263]. Problème pour les tokens très proches (High frequency) [cite: 264],we multiply the position m by L/L′[cite: 263]. Problem for very close tokens (High frequency) [cite: 264]
Ntk aware,on remplace la fréquence par une constante dépendante du ratio L/L′[cite: 264]. Out of band (on peut faire plus de 1 tour) + on traite toutes les dimensions pareil alors que le NN non [cite: 264],we replace the frequency with a constant dependent on the ratio L/L′[cite: 264]. Out of bond (we can do more than 1 rotation) + we treat all dimensions the same while the NN does not [cite: 264]
Ntk by part,condition sur la wavelength (valeur pour laquelle on fait une full rotation donc 2π/θ) et L/wave donne une condition pour si on doit atténuer θ ou non lors du calcul du RoPE [cite: 264],"condition on the wavelength (value for which we do a full rotation, so 2π/θ) and L/wave gives a condition on whether to attenuate θ or not during the RoPE calculation [cite: 264]"
Yarn (complet),ntk by part et division dans le softmax par un constante qui dépend du ratio L/L′ [cite: 264],Ntk by part and division in the softmax by a constant that depends on the ratio L/L′ [cite: 264]
Dual Chunk attention (DCA),"Le RoPE c'est entre q et k et ça fait une rotation m−n. Quand m−n est trop grand le modèle n'est pas entraîné. [cite: 264]DCA fait des chunks de taille 3/4 du max entraîné. [cite: 265]Si le token est distant relativement de moins d'un chunk : distance relative normale. [cite: 266]Si les tokens sont dans des chunks adjacents : distance relative capée sur la distance max −1. [cite: 266]Si les tokens sont dans des chunks non adjacents, gradient capé (on a un éloignement fixe maximal qui descend sur la valeur de k seulement) [cite: 266]","RoPE is between q and k and performs an m−n rotation. When m−n is too large, the model is not trained. [cite: 264]DCA uses chunks of size 3/4 of the max trained size. [cite: 265]If the token is relatively less than one chunk away: normal relative distance. [cite: 266]If the tokens are in adjacent chunks: relative distance capped at max distance −1. [cite: 266]If tokens are in non-adjacent chunks, capped gradient (we have a fixed maximal separation that only decreases on the value of k) [cite: 266]"
Qwen2,"classique Yarn Dual Chunk Attention, ORM, création d'instruction à partir de réponses de haute qualité, prompt avec guideline puis tu lui apprend sans les guidelines. [cite: 266]Le MoE a 8 experts actifs toujours et 8 partis sur 64 [cite: 267]","Classic Yarn Dual Chunk Attention, ORM, instruction creation from high-quality responses, prompt with guideline then you train it without the guidelines. [cite: 266]The MoE always has 8 active experts out of 64 total [cite: 267]"
Jina (XLM-RoBERTa),XLM-RoBERTa RoPE avec un fine-tuning LoRA pour les différentes tâches et du two towers avec 2 adapters pour query et passage. [cite: 267]Par contre reranker pas top [cite: 268],"XLM-RoBERTa RoPE with LoRA fine-tuning for different tasks and two towers with 2 adapters for query and passage. [cite: 267]However, reranker is not great [cite: 268]"
Itérations of thought,un inner dialogue agent juge et gère le dialogue et les reformulations. [cite: 268]Un LLM agent répond aux requêtes et améliorations du IDA [cite: 269],an inner dialogue agent (IDA) judges and manages the dialogue and reformulations. [cite: 268]An LLM agent responds to requests and IDA improvements [cite: 269]
Molmo,"CLIP + Qwen, aucun freezer pour le pretraining. [cite: 269]Dataset humain avec le captions en speech pour être efficace. [cite: 270]Dataset et fine-tuning pour pointer sur une image [cite: 271]","CLIP + Qwen, no freezing for pretraining. [cite: 269]Human dataset with captions as speech for efficiency. [cite: 270]Dataset and fine-tuning for pointing on an image [cite: 271]"
Logic of thought,"extraire les éléments logiques d'un prompt, utiliser un programme ou des logiques pour en déduire quelque chose, parser ça avec un LLM comme réponse [cite: 271]","extract logical elements from a prompt, use a program or logic to deduce something, parse that with an LLM as a response [cite: 271]"
Rag and beyond survey,"il y a 4 types d'applications RAG et il faut savoir ce qu'on veut faire[cite: 271]: Explicit facts retrieval; Implicite facts (graph, tree, itérative, text 2 sql) il faut faire des liens; Interprétation (prompt, COT, il faut réfléchir et interpréter); Hidden rationals (office learning utiliser un LLM pour déduire des choses, in context learning i.e. few shot et fine-tuning [cite: 272]) il faut se servir d'événements passés pour en sortir une logique exemple analyse d'incidents [cite: 272]","there are 4 types of RAG applications and you need to know what you want to do[cite: 271]: Explicit facts retrieval; Implicit facts (graph, tree, iterative, text 2 sql) links must be made; Interpretation (prompt, COT, reflection and interpretation are required); Hidden rationals (office learning use an LLM to deduce things, in context learning i.e. few shot and fine-tuning [cite: 272]) past events must be used to extract a logic, e.g., incident analysis [cite: 272]"
Making llm text embedders,on instruct un LLM avec few shot query passage puis on donne une query et on fine tune sur la NCE sur le token end of séquence [cite: 272],"we instruct an LLM with few-shot query passage, then we give a query and fine-tune on NCE on the end-of-sequence token [cite: 272]"
Colpali,"Colbert sur les patchs avec un LLM pour avoir token par token[cite: 272]. Late interaction : on regarde le maximum de similarité entre un token de text et chaque patch de l'image puis on somme ce max sur l'ensemble des tokens de text de la query, cela donne le LI entre la query et l'image. On a après une matrice de la taille query pages [cite: 273]","Colbert on patches with an LLM to have token by token[cite: 272]. Late interaction: we loat the maximum similarity between a text token and each image patch then we sum this max over all text tokens in the query, which gives the LI between the query and the image. We then have a matrix of size query pages [cite: 273]"
Slora,"partage de beaucoup d'adapter grâce à un batching, un load depuis la RAM et un tenseur parallèle [cite: 273]","sharing many adapters thanks to batching, loading from RAM, and parallel tensor [cite: 273]"
Where rnn all we needed,"min Lstm et min gru - on enlevé quasi toutes les récurrences sauf pour la sortie, on enlevé tous les tanh et grâce à un algo la récurrence est parallélisable quand c'est dans le format restant (parallel scan) [cite: 273]","min LSTM and min GRU - almost all recurrences removed except for the output, all tanh removed, and thanks to an algorithm the recurrence is parallelizable when it is in the remaining format (parallel scan) [cite: 273]"
Llms Know more than they show,"à partir des tokens qui font vraiment la réponse et des logits des layers d'avant (extraits avec un autre call LLM malheureusement) il est possible de classifier si la réponse est juste ou fausse[cite: 273, 274]. Probing classifier [cite: 274]","from the tokens that actually form the answer and the logits of the previous layers (extracted with another LLM call unfortunately) it is possible to classify whether the answer is right or wrong[cite: 273, 274]. Probing classifier [cite: 274]"
Contextuel document embedding,un peu comme un truc avec un vision encoder on rajoute au niveau de l'embedding les infos des autres documents proches + Entraînement sur des données de même cluster pour rendre les batchs plus difficiles [cite: 274],"a bit like a vision encoder thing, we add the info from other close documents to the embedding + Training on data from the same cluster to make batches more difficult [cite: 274]"
Nouveau modèle vision aria,"66 MoE plus petits que les FFN avec 2 communs et 6 choisis, la dim interne des experts est plus petite que la dim hidden, ViT avec learnable latent queries par un adapter en sortie. [cite: 274]Le modèle a des experts visuels selon la modalité dans chaque couche [cite: 275]","66 MoE smaller than FFNs with 2 common and 6 selected, the internal dimension of the experts is smaller than the hidden dimension, ViT with learnable latent queries via an output adapter. [cite: 274]The model has visual experts according to the modality in each layer [cite: 275]"
Pixtral,"RoPE 2D, break et end images [cite: 275]","RoPE 2D, break and end images"
Nvidia judge et dataset (Bradley Terry / SteerLM),"Bradley Terry (LLM + dense qui écarte le reward d'un win et loose end of séquence) et SteerLM régression (LLM + dense end of séquence qui prédit parmi 4 score), en réalité BT avec le base LLM qui est un SteerLM est le meilleur reward. [cite: 275] Les sets jugés par GPT-4o favorisent des sets de data de GPT-4o. Reinforce meilleur que PPO meilleur que DPO. [cite: 276]","Bradley Terry (LLM + dense that spreads the reward of a win and loose end of sequence) and SteerLM regression (LLM + dense end of sequence that predicts among 4 scores), in reality BT with the base LLM which is a SteerLM is the best reward. [cite: 275] Sets judged by GPT-4o favor GPT-4o data sets. Reinforce better than PPO better than DPO. [cite: 276]"
Model mergin différentiable adaptive merging,"Merge les modèles en les sommant avec un coefficient par modèle par layer par colonne. [cite: 277]Ces coefficients sont appris en minimisant une loss basée sur la KL divergence des logits entre le modèle merge et le modèle spécialisé sur i avec le dataset de i (avec un dataset et un modèle pour chaque type de modèle), L1 et L2 et on essaie aussi de minimiser la cosine distance entre les coefficients de chaque modèle pour aligner dans le même sens. [cite: 278]L'embedding et la RMS restent ceux du base modèle non spécialisé. [cite: 279]Il faudrait faire QLoRA adapter merging maintenant [cite: 280]","Merge models by summing them with a coefficient per model per layer per column. [cite: 277]These coefficients are learned by minimizing a loss based on the KL divergence of the logits between the merged model and the model specialized on i with the dataset of i (with one dataset and one model for each model type), L1 and L2, and we also try to minimize the cosine distance between the coefficients of each model to align them in the same direction. [cite: 278]The embedding and RMS remain those of the non-specialized base model. [cite: 279]QLoRA adapter merging should be done now [cite: 280]"
Allegro,"video VAE bien fait (temporal et res net), bonnes données, video DiT (self attention avec 3D RoPE, cross avec text et temporal avec Adaln) prédire le noise dans le latent [cite: 280]","well-made video VAE (temporal and res net), good data, video DiT (self attention with 3D RoPE, cross with text and temporal with Adaln) predict noise in the latent [cite: 280]"
Adaln,une couche de scale et shift de conditioning ajoutée avant chaque bloc d'attention [cite: 281],a scale and shift conditioning layer added before each attention block [cite: 281]
Réduction attention layer,similarité entre la sortie et l'entrée pour voir si c'est utile. [cite: 281]Tu enlèves la moitié des couches c'est quasi pareil [cite: 281],"similarity between output and input to see if it's useful. [cite: 281]You remove half the layers, it's almost the same [cite: 281]"
Rewarding progress scaling automated process,"on mesure la progression dans le process plutôt que la justesse de l'étape, un autre modèle weak mais pas trop c'est mieux pour ça et avec du beam search [cite: 281]","we measure progress in the process rather than the correctness of the step, another model that's weak but not too much is better for this, and with beam search [cite: 281]"
AttnLRP,layer wise relevance propagation [cite: 282],layer wise relevance propagation [cite: 282]
Spéculative decoding distillation,plutôt que d'avoir une ground truth ou les logits du teacher. [cite: 282]On laisse le student générer en mode spéculative et si le token généré n'est pas dans le top k du teacher on le rétribue et on continue. [cite: 282]Ça crée un dataset proche des 2 distributions [cite: 283],"rather than having a ground truth or the teacher's logits. [cite: 282]We let the student generate in speculative mode, and if the generated token is not in the teacher's top k, we reward it and continue. [cite: 282]This creates a dataset close to the 2 distributions [cite: 283]"
Arithmetic without algorithm,sur des opérations simples (+×−) un très petit nombre de neurones par layer est responsable du résultat. [cite: 283]Ils correspondent à des heuristiques (pour une opération et une range de réponse). [cite: 284]La solution d'opérations vient d'une combinaison de ces heuristiques [cite: 285],on simple operations (+×−) a very small number of neurons per layer is responsible for the result. [cite: 283]They correspond to heuristics (for an operation and a range of answer). [cite: 284]The solution to operations comes from a combination of these heuristics [cite: 285]
Longrag,Retriever et rerank[cite: 285]. Utiliser un LLM pour extraire l'info globale du chunk associé à un long contexte autour du chunk - info 1. [cite: 285]Sélectionner le chunk avec un COT qui décide si ça va répondre ou pas - info 2 [cite: 285],Retriever and rerank[cite: 285]. Use an LLM to extract global info from the chunk associated with a long context around the chunk - info 1. [cite: 285]Select the chunk with a COT that decides if it will answer or not - info 2 [cite: 285]
Modèle Tencent,"MoE 16 experts, 1 commun + 1 activé. [cite: 286] Cross layer attention toutes les 2 couches. RoPE et SwiGLU. On policy et off policy DPO. [cite: 286]Beaucoup de synthétique [cite: 287]","MoE 16 experts, 1 common + 1 activated. [cite: 286] Cross-layer attention every 2 layers. RoPE and SwiGLU. On-policy and off-policy DPO. [cite: 286]Lots of synthetic data [cite: 287]"
Nanny ml metrics without ground truth,,
L'autre colpali unifying,"pas de late interaction, LLM phi et CLIP [cite: 287]","no late interaction, LLM phi and CLIP [cite: 287]"
Docling (Table former / Layout),"Table former : modèle transformer d'extraction de tables avec un décoder pour les tags (structure) et un pour les box (extraction du contenu)[cite: 287]. Layout : basé sur RT-DETR qui est YOLO en mieux : suite de CNN pour extraire différentes features, AIFI fusion pour meilleure information, fusion des features cross channel qui deviennent des queries puis transformer pour extraire les plus importantes et têtes de détection [cite: 287]","Table former: transformer model for table extraction with one decoder for tags (structure) and one for boxes (content extraction)[cite: 287]. Layout: based on RT-DETR which is YOLO but better: sequence of CNNs to extract different features, AIFI fusion for better information, fusion of cross-channel features that become queries then transformer to extract the most important ones and detection heads [cite: 287]"
Mixture of transformer,"On a un transformer par modalité. On classe la séquence par modalité (text, image, speech) on fait les projections Q,V, K, on réordonne pour la formule de l'attention. [cite: 288]On reclasse par modalité puis on a un feedforward par modalité et on ordonne de nouveau. [cite: 289]On peut même avoir différentes loss par transformer du coup (diffusion vs autoregressif) [cite: 290]","We have one transformer per modality. We classify the sequence by modality (text, image, speech), we make the Q, V, K projections, we reorder for the attention formula. [cite: 288]We reclassify by modality then we have a feedforward per modality and we order again. [cite: 289]We can even have different losses per transformer (diffusion vs autoregressive) [cite: 290]"
Pddl (Planning Domain Definition Language),"pour planning et API plusieurs soucis : nombre d'API limité, hallucinations, query pas complète pour les clés, planning difficile. [cite: 290]LLM qui génère une représentation de la query en ASP puis PDDL via le solver qui fait le pont entre sémantique et API (Planning Domain Definition Language) avec un PDDL qui représente le domaine (action possible input et output) et un pour la tâche[cite: 291]. On résout ça avec un solver classique IA donc pas d'hallucinations etc [cite: 291]","for planning and API several issues: limited number of APIs, hallucinations, incomplete query for keys, difficult planning. [cite: 290]LLM that generates a representation of the query in ASP then PDDL via the solver which bridges semantics and API (Planning Domain Definition Language) with one PDDL representing the domain (possible actions, input and output) and one for the task[cite: 291]. This is solved with a classic AI solver, so no hallucinations, etc. [cite: 291]"
Explainability pour Molmo,(en utilisant CLIP) [cite: 291],(using CLIP) [cite: 291]
Taxonomy agentops,,
Qwen coder report,"Qwen 2.5 en base, data code, text et math avec NTP et fill in the Middle (on donne le début et la fin d'un code et il fait le milieu avec des tokens spéciaux pour lui dire). [cite: 292]RoPE et augmentation de la fréquence pour long contexte sur des repos + Yarn [cite: 292]","Qwen 2.5 as base, code, text, and math data with NTP and Fill-in-the-Middle (we give the start and end of a code and it generates the middle with special tokens to instruct it). [cite: 292]RoPE and frequency increase for long context on repos + Yarn [cite: 292]"
Nvidia paper ssm et attention,meta token pour stocker l'info (différents pour différents domaines). [cite: 293] SSM et attention qui sont pool. Sliding Window sauf pour 3 layers pour accélérer car SSM a déjà le global. [cite: 294] Cross layer attention. L'attention se concentre sur le cross et le SSM sur le self [cite: 295],meta token to store info (different for different domains). [cite: 293] SSM and attention are pooled. Sliding Window except for 3 layers to speed up because SSM already has the global view. [cite: 294] Cross-layer attention. Attention focuses on cross and SSM on self [cite: 295]
Tulu,"Llama 3.1 base model[cite: 295]. Gros job sur la data, SFT avec un focus sur des skills (math, code, recall, instruct), DPO on policy et off policy avec d'autres models + GPT-4o qui score les préférences, et RL with vérifiable reward avec maths et précise instructions avec PPO et un vérifiable. [cite: 295]Grosse étude des overlap des datasets entre train et eval avec ngram. [cite: 296]Génération de data avec des persona (250k persona). [cite: 296]SFT skill specific model pour ajuster le dataset et savoir quel dataset est bien pour quel skill. [cite: 296]Quand on veut entraîner un plus gros modèle on augmente le batch size et diminue le LR. [cite: 296]Framework d'évaluation complet [cite: 296]","Llama 3.1 base model[cite: 295]. Big job on the data, SFT with a focus on skills (math, code, recall, instruct), DPO on-policy and off-policy with other models + GPT-4o scoring preferences, and RL with verifiable reward with math and precise instructions with PPO and a verifiable reward. [cite: 295]Major study of dataset overlap between train and eval with ngram. [cite: 296]Data generation with personas (250k personas). [cite: 296]SFT skill specific model to adjust the dataset and know which dataset is good for which skill. [cite: 296]When training a bigger model, we increase the batch size and decrease the LR. [cite: 296]Complete evaluation framework [cite: 296]"
Flux,"diffuser classique avec latent pour le noise, scheduler pour savoir les timestamp, T5 encoder pour le text mais CLIP pour ajouter au scheduler pour pas mal d'étapes de modulation. [cite: 296]N composants de multimodal (en gros on mélange text et image en faisant du point wise de q,k et v donc c'est pas de la cross attention) avec une étape de modulation par modalité. [cite: 297]Puis N composants de self attention normale du truc assemblé. [cite: 298]QWQ : principalement entraîné sur du COT [cite: 299]","Classic diffuser with latent for noise, scheduler to know the timestamp, T5 encoder for text but CLIP to add to the scheduler for many modulation steps. [cite: 296]N multimodal components (basically mixing text and image by doing point-wise q,k, and v, so it's not cross-attention) with a modulation step per modality. [cite: 297]Then N normal self-attention components of the assembled thing. [cite: 298]QWQ: mainly trained on COT [cite: 299]"
Qwen2vl,"Naive dynamic résolution : process toutes les résolutions et vidéos possibles avec 2D RoPE dans le ViT + compression après patching avec un MLP pour 2×2=1 token[cite: 299]. Multimodal RoPE : 3 dimensions et incrémentation de 1 pour le changement de modalité du maximum des 3 modalités[cite: 299]. Vidéo : 2 frames par secondes sampled, avec une limite de token par vidéos qui influe sur la compression et 3D convolution (dans le ViT ???) [cite: 299]","Naive dynamic resolution: processes all possible resolutions and videos with 2D RoPE in the ViT + compression after patching with an MLP for 2×2=1 token[cite: 299]. Multimodal RoPE: 3 dimensions and incrementation by 1 for modality change of the maximum of the 3 modalities[cite: 299]. Video: 2 frames per second sampled, with a token limit per video that influences compression and 3D convolution (in the ViT ???) [cite: 299]"
Autoround quantization,quantize et dequantize avec une formule qui clip avec un param s qui dépend de α et β [cite: 300]et v qui sont appris sur l'erreur de dequantization [cite: 300],quantize and dequantize with a formula that clips with a param s that depends on α and β [cite: 300]and v which are learned on the dequantization error [cite: 300]
Gaia,"leader board de 400 questions classées qui nécessite des tools. [cite: 300]Tools : Web browsing, multimodality tool (STT, OCR, ), coding, file read, autre [cite: 301]","leaderboard of 400 classified questions that require tools. [cite: 300]Tools: Web browsing, multimodality tool (STT, OCR, ), coding, file read, other [cite: 301]"
Yi lightbing,classique MoE [cite: 301],classic MoE [cite: 301]
Chain of continious thought,le COT se fait en latent pour avoir beaucoup plus d'informations et revient direct en input sans skipper le tokenizer. [cite: 301]Fixed size thinking et k steps de thinking [cite: 302],COT is done in latent space to have much more information and goes directly back into the input without skipping the tokenizer. [cite: 301]Fixed size thinking and k thinking steps [cite: 302]
Process bench,,
Meta no token,"On travaille au niveau des bytes avec un embedding auquel on ajoute les hash des ngrams précédents. [cite: 302]On fait de la cross attention pour représenter les patchs (key et values) de manière perceiver resampler. [cite: 303]Ces groupements de bytes sont déterminés à partir d'un petit self transformer et de la perte d'entropie sur la tâche de next byte prediction. [cite: 304]Ensuite on passe dans un petit encoder qui fait le perceiver avec une couche self et une couche cross avec les vecteurs patchs qui sont initialisés avec un max pooling des embedding des bytes du patch pour représenter les groupes de bytes en patch et en sortie de l'encodeur, les patchs sont envoyés dans un transformer classique puis décodés avec sensiblement le même système inverse (cross attention puis transformer)[cite: 305]. Prédiction de 1 byte ou d'une séquence de bytes ??? [cite: 305]Regarder le code pour voir ce qui est décodé et à quel moment c'est patché [cite: 306]","We work at the byte level with an embedding to which we add the hashes of the previous ngrams. [cite: 302]We use cross-attention to represent the patches (key and values) in a perceiver resampler manner. [cite: 303]These byte groupings are determined from a small self-transformer and the entropy loss on the next byte prediction task. [cite: 304]Then we pass through a small encoder that performs the perceiver with a self layer and a cross layer with the patch vectors that are initialized with a max pooling of the byte embeddings of the patch to represent the byte groups in a patch, and at the encoder output, the patches are sent to a classic transformer then decoded with essentially the same inverse system (cross-attention then transformer)[cite: 305]. Prediction of 1 byte or a sequence of bytes??? [cite: 305]Check the code to see what is decoded and when it is patched [cite: 306]"
Jina clip v2,"XML roberta et image encoder Eva02, train en 3 stages sur de l'info NCE avec hard negative et long captions au stage 3, full multilingue avec le dataset Vidore en particulier. [cite: 306]Pas au niveau de Colpali mais ça va [cite: 307]","XLM-RoBERTa and image encoder Eva02, trained in 3 stages on NCE info with hard negatives and long captions at stage 3, fully multilingual with the Vidore dataset in particular. [cite: 306]Not at Colpali's level but it's fine [cite: 307]"
Large concept model,"Sonar qui encode et décode des phrases[cite: 307]. LCM qui génère les concepts (les représentations des phrases) - decoder only avec une MSE loss, diffusion pour avoir un espace continu [cite: 307]","Sonar that encodes and decodes sentences[cite: 307]. LCM that generates the concepts (sentence representations) - decoder only with an MSE loss, diffusion to have a continuous space [cite: 307]"
Apollo llm,"SigLIP image encoder et InternVideo2 vidéo encoder concat et passés dans un perceiver puis Qwen. [cite: 308]Ablation sur les types de data, les encoder, le sampling des vidéos. [cite: 308]Gros benchmark modèle vidéos [cite: 308]","SigLIP image encoder and InternVideo2 video encoder concatenated and passed into a perceiver then Qwen. [cite: 308]Ablation on data types, encoders, video sampling. [cite: 308]Big video model benchmark [cite: 308]"
Nvlm (Nvidia VLM),"ablation sur la méthode de jointure des patchs et du LLM (cross encoder, MLP, ...) découpage de l'image avec thumbnail [cite: 308]","ablation on the method for joining patches and the LLM (cross encoder, MLP, ...) image slicing with thumbnail [cite: 308]"
Adam vs AdamW,Weight decay = L2 directement appliquée au param et multipliée par le LR alors que Adam n'a pas le LR sur L2 cela permet de ne pas avoir de trop gros poids d'être stable et généraliste. [cite: 309]Momentum = prise en compte des gradients d'avant pour convergence plus rapide et moins de bruit. [cite: 310]Velocité = prise en compte du carré des gradients pour adapter le learning rate. [cite: 311]Clipping on empêche la norme de gradient de dépasser 1 pour ne pas exploser[cite: 311]. Linéar warmup : on monte le LR petit à petit pour pas apprendre trop vite et garder un optimum[cite: 311]. Cosine decay : on diminue jusqu'à 0 le LR sur la période pour continuer petit à petit à converger sans rater l'optima et arriver chill à la fin sans tout bouger [cite: 311],"Weight decay = L2 directly applied to the parameter and multiplied by the LR, while Adam doesn't have the LR on L2, which helps to avoid having too large weights, to be stable and generalist. [cite: 309]Momentum = takes previous gradients into account for faster convergence and less noise. [cite: 310]Velocity = takes the square of the gradients into account to adapt the learning rate. [cite: 311]Clipping prevents the gradient norm from exceeding 1 to avoid explosion[cite: 311]. Linear warmup: the LR is gradually increased to avoid learning too fast and maintain an optimum[cite: 311]. Cosine decay: the LR is decreased to 0 over the period to continue converging gradually without missing the optima and arriving chill at the end without moving everything [cite: 311]"
Loss ocr (CTC),CTC avec −log de la proba. [cite: 312]On a un token blank qui permet au modèle de prédire rien quand il n'a pas toutes les infos. [cite: 313]Ensuite on supprime les blank et les répétitions qui ne sont pas séparées par blank. [cite: 314]La CTC loss prend en compte les différents chemins qui peuvent amener à la bonne prédiction (donc avec les blancs et répétitions) [cite: 314],"CTC with −log of the probability. [cite: 312]We have a blank token that allows the model to predict nothing when it doesn't have all the info. [cite: 313]Then we remove blanks and repetitions not separated by blank. [cite: 314]The CTC loss accounts for different paths that can lead to the correct prediction (i.e., with blanks and repetitions) [cite: 314]"
Crnn,conv pour avoir 1 ou 2 de hauteur et les channels et la largeur de l'image. [cite: 315]Puis split par colonne qu'on envoie à un RNN + FNN + softmax qui prédit le caractère ou un blank pour chaque pas de séquence [cite: 315],Conv to get 1 or 2 in height and channels and image width. [cite: 315]Then split by column that is sent to an RNN + FNN + softmax which predicts the character or a blank for each sequence step [cite: 315]
Cohere (Architecture),3 couches avec sliding window et RoPE et une couche avec full séquence sans positional encoding [cite: 315],3 layers with sliding window and RoPE and one layer with full sequence without positional encoding [cite: 315]
Q former = query former,"on a des query latent comme pour le perceiver resampler sauf que on a 2 modules : un avec la query latent, self attention et cross attention avec un image encoder et un pur encoder avec comme input le texte correspondant à l'image. [cite: 316]Ensuite on apprend en constructive à avoir des latents qui en sortie de Q former ressemblent à la sortie du texte [cite: 316]","we have latent queries like for the perceiver resampler except that we have 2 modules: one with the latent query, self-attention, and cross-attention with an image encoder, and a pure encoder with the text corresponding to the image as input. [cite: 316]Then we learn constructively to have latents at the output of Q former that resemble the text output [cite: 316]"
Perceiver resampler,"transformer avec des latents : q c'est le latent, k et v c'est latent concat avec ce qu'on veut représenter [cite: 316]","transformer with latents: q is the latent, k and v is latent concat with what we want to represent [cite: 316]"
Wave2vec,Des couches de CNN avec activation GeLU pour faire un latent (512 channels)[cite: 317]. Un transformer pour représenter l'embedding[cite: 317]. Puis entraînement self supervised : on mask 50% des latent et on fait passer ça dans un quantizer qui les représente de façon discrète. [cite: 317]On sample 100 latent quantize comme négative et le bon comme positive et on fait constrative loss entre la sortie du transformer et ceux-là [cite: 317],CNN layers with GeLU activation to create a latent (512 channels)[cite: 317]. A transformer to represent the embedding[cite: 317]. Then self-supervised training: we mask 50% of the latent and pass it through a quantizer which represents them discretely. [cite: 317]We sample 100 quantized latents as negative and the correct one as positive and we use contrastive loss between the transformer output and those [cite: 317]
Test time Training,"dataset ARC, permutation pour augmenter et fine-tuning LoRA sur la tâche spécifique augmente beaucoup les perfs [cite: 317]","ARC dataset, permutation for augmentation and LoRA fine-tuning on the specific task greatly increases performance [cite: 317]"
Deepseek v3,"MoE : shared expert et gating avec un sigmoid(vecteur×layer de classification d’expert) et softmax pour l'activation des experts du top k (donc en gros leur impact est sommé avec un poids) les 3 premières couches ne sont pas MoE. [cite: 318]Par contre une limite de 4 nodes max par token (mais 8 experts)[cite: 318]. Ajout d'un biais dans le calcul de la décision des experts pour équilibrer les experts en training[cite: 318]. Multi latent attention : on stocke le KV cache dans un vecteur latent commun beaucoup plus petit qu'on découpe en k et v avec une couche linéaire chacun, ça réduit de beaucoup le cache - 512 vs 2×7168 (61 layers)[cite: 318]. Multi token prediction : par token qu'on veut prédire en multiple on concat la sortie du modèle à l'embedding on fait passer dans un transformer et on prédit le token. [cite: 319]On stack ça de façon itérative pour k token avec une loss sur le i-ème token par simple block. [cite: 320]Ça permet le Spéculative decoding mais aussi de sensibiliser le modèle à penser beaucoup plus loin. [cite: 321]En mode normal on enlève juste ces couches et le modèle s'utilise normalement [cite: 321]","MoE: shared expert and gating with a sigmoid(vector×expert classification layer) and softmax for activation of the top k experts (so basically their impact is summed with a weight), the first 3 layers are not MoE. [cite: 318]However, a limit of 4 max nodes per token (but 8 experts)[cite: 318]. Addition of a bias in the expert decision calculation to balance experts during training[cite: 318]. Multi latent attention: the KV cache is stored in a much smaller common latent vector, which is split into k and v with a linear layer each, which greatly reduces the cache - 512 vs 2×7168 (61 layers)[cite: 318]. Multi token prediction: for multiple tokens we want to predict, we concatenate the model output to the embedding, pass it through a transformer, and predict the token. [cite: 319]This is stacked iteratively for k tokens with a loss on the i-th token per simple block. [cite: 320]This allows for Speculative decoding but also sensitizes the model to think much further ahead. [cite: 321]In normal mode, these layers are just removed, and the model is used normally [cite: 321]"
Olmo 2,"pre Training, mid Training (phase d'annealing répétées sur des petits set de domain specific amélioré beaucoup) pas mal de travail sur les weight decay (enlever l'embedding de ça améliore) sur les spike loss (RMS juste après l'attention et avant le résiduel est mieux que LN sur la sortie) Z norm, no bias, KVQ norm. [cite: 322]Model souping est bien[cite: 322]. Tulu3 pour post Training. SFT avec environ 1M prompt, DPO avec un pool de 20 models open et GPT-4o qui fait le juge pour noter les préférences puis select la meilleure comme w et sample une des autres pour l. [cite: 323] (On et off car dans les 20 il y a des checkpointing Olmo). [cite: 324]Ils donnent tous les paramètres de PPO [cite: 324]","Pre-training, mid-training (repeated annealing phase on small domain-specific sets greatly improved) a lot of work on weight decay (removing the embedding from it improves) on spike loss (RMS right after attention and before the residual is better than LN on the output) Z norm, no bias, KVQ norm. [cite: 322]Model souping is good[cite: 322]. Tulu3 for post-training. SFT with around 1M prompts, DPO with a pool of 20 open models and GPT-4o acting as the judge to rate preferences, then select the best as w and sample one of the others as l. [cite: 323] (On and off because Olmo checkpoints are in the 20). [cite: 324]They give all PPO parameters [cite: 324]"
Smaller Weaker yet better,"à budget égal, il vaut mieux avoir plus de samples quand on fait du sampling sur la réponse d'un plus petit modèle que plus d'un gros modèle (coverage et diversité) [cite: 324]","for the same budget, it is better to have more samples when sampling from the response of a smaller model than more from a larger model (coverage and diversity) [cite: 324]"
Model 7b et prm rstar math,"un petit modèle génère des steps de solution avec MCTS et donc q value avec un PPM (il est entraîné sur des préférences basées sur ses q values plutôt que sur un label - on sample pour une même étape la plus positive et la plus négative plutôt que tout classer et Bradley Terry [cite: 325] pour la loss). Tout le training est en self improve de la policy et critique (PPM) en 4 rounds ça bat O1 en maths. [cite: 325] Les q value sont initialisées avec un terminal guided annotation. Ablation sur ORM, PRM avec q value et PPM. [cite: 326]PPM est meilleur. [cite: 327]","a small model generates solution steps with MCTS and therefore q value with a PPM (it is trained on preferences based on its q values rather than on a label - for the same step, we sample the most positive and most negative rather than classifying everything and Bradley Terry [cite: 325] for the loss). The entire training is a self-improvement of the policy and critic (PPM); in 4 rounds it beats O1 in math. [cite: 325] The q values are initialized with a terminal guided annotation. Ablation on ORM, PRM with q value, and PPM. [cite: 326]PPM is better. [cite: 327]"
Reinforce ++,GRPO avec pénalité KL sur le reward (le reward est la KL avec l'ajout du reward sur le dernier token) [cite: 327],GRPO with KL penalty on the reward (the reward is the KL with the addition of the reward on the last token) [cite: 327]
Kalm,"travail sur la data, clearing des false negative, hard negative mining, mean pooling et train sur 512 mais RoPE donc long [cite: 327]","work on the data, clearing false negatives, hard negative mining, mean pooling, and train on 512 but RoPE so long context [cite: 327]"
TabPFN,,
Qwen prm,"combinaison de Monte Carlo et LLM as judge pour sélectionner parmi n samples d'une step. [cite: 328]Évaluer sur best of n force le ORM donc Process Bench est mieux. [cite: 328]Monte Carlo: we position ourselves at step t and predict k responses from there up to the final answer. [cite: 329]The MC value is the proportion of correct answers among the generated responses, so the power of this step to generate a good response [cite: 330]","combination of Monte Carlo and LLM as judge to select among n samples of a step. [cite: 328]Evaluating on best of n forces ORM so Process Bench is better. [cite: 328]Monte Carlo: we position ourselves at step t and predict k responses from there up to the final answer. [cite: 329]The MC value is the proportion of correct answers among the generated responses, so the power of this step to generate a good response [cite: 330]"
Lightning attention,"7 layers sur 8 = la combinaison permet un super retrieval, un long contexte et une rapidité quasi linéaire [cite: 330]","7 layers out of 8 = the combination allows for excellent retrieval, long context, and almost linear speed [cite: 330]"
Deepseek r1 zero,"pas de SFT, GRPO direct sur base model avec accuracy reward math et code et format reward sur le thinking. Pas de PRM. [cite: 331]En pur RL, le modèle tend naturellement vers des réponses plus longues et réfléchies, révision et réévaluation se fait naturellement [cite: 332]","no SFT, GRPO direct on base model with math and code accuracy reward and format reward on thinking. No PRM. [cite: 331]In pure RL, the model naturally tends towards longer and more thoughtful responses, revision and re-evaluation happen naturally [cite: 332]"
Deepseek r1 avec cold start,"short SFT avec COT, RL sur clear solution (math, code,...)+ lang consistency reward, puis SFT sur des données générales avec rejection sampling avec v3 + v3 dataset pour non reasoning data + final RL classic de v3. [cite: 333]Distillation en pur SFT ??? [cite: 334]Distiller d'un gros est mieux que de faire la pipeline complète [cite: 334]","short SFT with COT, RL on clear solution (math, code,...)+ lang consistency reward, then SFT on general data with rejection sampling with v3 + v3 dataset for non-reasoning data + final classic RL of v3. [cite: 333]Distillation in pure SFT ??? [cite: 334]Distilling from a large model is better than doing the complete pipeline [cite: 334]"
Kimi paper 16p (Multimodal Reasoning),multimodal reasoning model avec RL simple type Reinforce avec moyenne des rewards sur base model et FT COT. [cite: 335]Graduation en difficulté et RM qui compare la réponse à la ground truth pour donner un scalar [cite: 335],multimodal reasoning model with simple RL like Reinforce with average rewards on base model and FT COT. [cite: 335]Graduation in difficulty and RM that compares the response to the ground truth to give a scalar [cite: 335]
Qwen moe balancing 9p,faire le LBL (Layer-wise Balancing Loss) au niveau du batch complet plutôt que post data parallel (donc au niveau proche de la séquence) permet d'équilibrer les experts beaucoup mieux et de les spécialiser fortement [cite: 335],doing the LBL (Layer-wise Balancing Loss) at the full batch level rather than post data parallel (so close to the sequence level) allows for much better expert balancing and strong specialization [cite: 335]
Bytedance agent 27p (Agent GUI),"agent GUI avec une approche data driven (+ouverture sur le RL FT suite à d'autres interactions à venir pour s'améliorer) vs design driven (prompt c'est trop chiant et trop peu sûr, frameworks trop prompts à l'erreur et peu fiables en prod). [cite: 336]Lien RPA, agent framework, native agent et lifeline agent plus tard. [cite: 337]Pas mal de tâches pour apprendre au modèle à cliquer (coordonnées) décrire et localiser, capter les changements et les milestone. [cite: 338] Technique de agent DPO : on garde les traces fausses et la trace successful pour DPO le modèle sur ça - online DPO. Ces data sont online [cite: 338]","GUI agent with a data-driven approach (+openness to RL FT following other future interactions to improve) vs design-driven (prompting is too annoying and too unreliable, frameworks too prone to error and unreliable in prod). [cite: 336]Link RPA, agent framework, native agent, and lifeline agent later. [cite: 337]Quite a few tasks to teach the model to click (coordinates) describe and localize, capture changes and milestones. [cite: 338] Agent DPO technique: we keep the false traces and the successful trace to DPO the model on this - online DPO. This data is online [cite: 338]"
Large action model,lu en diagonale / plan et task mais plutôt orienté GUI. [cite: 339]Un LLM vraiment tuné uniquement sur de l'action [cite: 339],skimmed / plan and task but rather GUI-oriented. [cite: 339]An LLM really tuned only on action [cite: 339]
Rl deepmind muzero,"RL à partir d'observation d'un jeu (genre x parties et Z observer) un modèle qui n'a pas les règles et qui fait juste un hidden state. [cite: 340]3 têtes - hidden state, policy, value et reward[cite: 340]. Utilisation avec MCTS et nombre de visite d'une node comme proba [cite: 340]","RL from game observations (e.g., x games and Z observed) a model that doesn't have the rules and just creates a hidden state. [cite: 340]3 heads - hidden state, policy, value, and reward[cite: 340]. Use with MCTS and number of node visits as probability [cite: 340]"
Qwen 2.5 max,"pretraining fill in the middle, position et word retrieval, paragraph reordering 5 training phases pour s'étendre petit à petit[cite: 340]. DCA (remap relative pos avec intra, inter et successive) et Yarn pour faire ×4[cite: 341]. Pour le prefilling Minference : on sélectionne seulement les token qui ont une attention avec les derniers token pour compute leur attention, grosse réduction. [cite: 341]On fait ça par chunk aussi (donc plusieurs lots de derniers) [cite: 341]","pretraining fill-in-the-middle, position and word retrieval, paragraph reordering 5 training phases to gradually extend[cite: 340]. DCA (remap relative pos with intra, inter and successive) and Yarn to do ×4[cite: 341]. For prefilling Minference: only tokens that have attention with the last tokens are selected to compute their attention, big reduction. [cite: 341]This is also done by chunk (so multiple batches of last ones) [cite: 341]"
Critique fine tuning,"plutôt que d'apprendre au modèle à imiter des réponses, on lui apprend à critiquer des réponses et donc à les comprendre [cite: 342]","rather than teaching the model to imitate responses, we teach it to critique responses and thus understand them [cite: 342]"
Sft memorize rl generalize,SFT a tendance à ne pas performer sur de l'OOD mais est nécessaire pour formater la réponse et donner une bonne base au RL qui lui après permet de généraliser à des tâches de language ou de vision [cite: 342],"SFT tends not to perform on OOD but is necessary to format the response and give a good base to RL, which then allows generalization to language or vision tasks [cite: 342]"
Janus pro paper,"plus gros modèle, plus de data et focus sur meilleure qualité [cite: 342]","bigger model, more data and focus on better quality [cite: 342]"
Janus et Vq tokenizer,"encoder décoder CNN qui réduit la dimension de l'image et amène dans un codebo(c'est une quantization du vecteur de feature dans un dictionnaire de tokens, on map le feature sur l'indice du token le plus proche). [cite: 343]On entraîne sur une loss qui a l'erreur de reconstruction et l'erreur de représentation du feature avec le vecteur quantizé. [cite: 343]Ensuite en mode autoreg il y a un MLP pour adapter le codeboà l'embedding et une prediction head sur le vocabulaire texte et une sur le vocabulaire image (codebook) [cite: 344]","CNN encoder-decoder that reduces the image dimension and leads into a codebo(it's a quantization of the feature vector into a dictionary of tokens, we map the feature to the index of the closest token). [cite: 343]Training is done on a loss that has the reconstruction error and the feature representation error with the quantized vector. [cite: 343]Then in autoregressive mode, there is an MLP to adapt the codeboto the embedding and one prediction head on the text vocabulary and one on the image vocabulary (codebook) [cite: 344]"
Joint embedding,"pas de contrastive loss mais L2 loss dans l'espace embedding. [cite: 345] Génération dans l'espace d'embedding. Embedding pour target et input en ViT, predictor en ViT [cite: 345]","no contrastive loss but L2 loss in the embedding space. [cite: 345] Generation in the embedding space. Embedding for target and input in ViT, predictor in ViT [cite: 345]"
On teacher hacking,"le fait de passer par un proxy teacher peut faire du hacking, on se rapproche du teacher mais s'éloigne de la réalité. [cite: 345]Il faut privilégier le online distillation, la diversité des prompts et si offline sampler plus de réponses. [cite: 346]LoRA colpali reranker avec un mini Qwen [cite: 347]","passing through a proxy teacher can lead to hacking; we get closer to the teacher but move away from reality. [cite: 345]Online distillation, prompt diversity, and if offline, sample more responses should be favored. [cite: 346]LoRA Colpali reranker with a mini Qwen [cite: 347]"
Scaling up test time compute with latent reasoning,"l'idée est de penser dans l'espace latent avec une récurrence sur le depth au lieu du temps. [cite: 347]Le modèle est composé de 3 blocs de couches transfo : prélude qui construit l'embedding, coda qui décode le dernier espace latent en tokens et un bloc récurrent qui prend un bruit blanc et l'embedding puis les espaces latents et l'embedding avec le nombre de récurrences variables. [cite: 348]Ça vient de la theory de deep thinking et le fait que dans les LLM la première et dernière couches sont très différents mais au milieu c'est échangeable[cite: 349]. Ils utilisent un adapter qui combine via concaténation et réduction l'espace latent et l'embedding dans un seul espace[cite: 349]. Travail sur le nombre de récurrence en train sample sur du poisson et sur la backprop qui doit être indep de r (donc backprop que sur les k dernières récurrences mais comme l'embedding est toujours concat, même si k alors le prélude est toujours backprop)[cite: 349]. Ils ont étudié le fait de partager le KV cache en early stopping [cite: 350]et de mettre un KV cache de taille définie en partageant avec un modulo sur le cache. [cite: 351]On peut aussi COT en prenant le bruit blanc à partir du dernier hidden state du dernier token[cite: 351]. On peut aussi Speculative decoding avec moins de steps et vérifier sur le batch avec plus de steps [cite: 351]","the idea is to think in the latent space with a recurrence on depth instead of time. [cite: 347]The model is composed of 3 blocks of transformer layers: prelude which builds the embedding, coda which decodes the last latent space into tokens, and a recurrent block which takes white noise and the embedding then the latent spaces and the embedding with a variable number of recurrences. [cite: 348]This comes from the theory of deep thinking and the fact that in LLMs the first and last layers are very different but the middle is interchangeable[cite: 349]. They use an adapter that combines via concatenation and reduction the latent space and the embedding into a single space[cite: 349]. Work on the number of recurrences in training sample on Poisson distribution and on backprop that must be independent of r (so backprop only on the last k recurrences but since the embedding is always concatenated, even if k, the prelude is still backprop)[cite: 349]. They studied sharing the KV cache with early stopping [cite: 350]and setting a fixed size KV cache by sharing with a modulo on the cache. [cite: 351]We can also do COT by taking white noise from the last hidden state of the last token[cite: 351]. We can also do Speculative decoding with fewer steps and verify on the batch with more steps [cite: 351]"
On the émergence of thinking in llm,thinking = search et stratégie. [cite: 351]COT amélioré le thinking et donc le reasoning et plus la réponse est longue meilleure est la réponse. [cite: 352]SFT sur du thinking et PPO avec 2 rewards : outcome vérifiable et thinking reward (un truc sur la longueur de la réponse et un LLM as a judge) [cite: 353],"thinking = search and strategy. [cite: 351]COT improves thinking and thus reasoning and the longer the response, the better the response. [cite: 352]SFT on thinking and PPO with 2 rewards: verifiable outcome and thinking reward (something about the length of the response and an LLM as a judge) [cite: 353]"
Infinitehip,prune les tokens pour k et v pour diminuer la taille et adaptation du RoPE [cite: 353],prunes tokens for k and v to reduce size and adapts RoPE [cite: 353]
Adaptative graph of thought,"11p d'âge decomposition récursivité et graph, passer plus de temps sur des sous-tâches complexes. [cite: 353]Alternative à COT et TOT [cite: 354]","11-page age decomposition recursion and graph, spending more time on complex sub-tasks. [cite: 353]Alternative to COT and TOT [cite: 354]"
Large diffusion model (Text),"8p transformer bidirectionnel entraîné comme un LLM sauf que l'idée est de masquer tous les mots et de démasquer petit à petit (diffusion) avec un remasquage sur les low confidence pour corriger et une prediction de la longueur de la réponse au début. [cite: 354]Pretraining tout token avec une proba masque, SFT token réponse avec une proba masque masquée, inférence prédit la taille + full mask et remasquage [cite: 355]","8-page bidirectional transformer trained like an LLM except the idea is to mask all words and gradually unmask (diffusion) with remasking on low confidence to correct and a prediction of the response length at the start. [cite: 354]Pretraining all tokens with a mask probability, SFT response tokens with a masked mask probability, inference predicts size + full mask and remasking [cite: 355]"
Deepseek nsa (Sparse Attention),16p sparse trained attention avec 3 composants qui sont combinés via un poids calculé par un MLP et softmax suite à leur calcul d'attention : [cite: 355]Compression : on découpe en blocs et on compresse avec un MLP. [cite: 355]Token sélection : on prend les tokens des top n blocs avec le plus gros softmax. [cite: 356]Sliding window [cite: 356],16-page sparse trained attention with 3 components that are combined via a weight calculated by an MLP and softmax following their attention calculation: [cite: 355]Compression: we cut into blocks and compress with an MLP. [cite: 355]Token selection: we take the tokens of the top n blocks with the largest softmax. [cite: 356]Sliding window [cite: 356]
Mixture of Block attention for long context,10p [cite: 356],10 pages [cite: 356]
Siglip v2,"10p Training avec une logistic loss, une dense loss qui est en fait un décoder qui classifie des régions et captions et une distillation loss sur du MLM et du EMA (en gros on montre un bout de l'image au student et l'image entière au teacher, on fait MLM avec ça aussi) plutôt que contrastive (donc ça en fait une tâche de classif pour la première) map head pour le pooling [cite: 356]","10-page training with a logistic loss, a dense loss that is actually a decoder which classifies regions and captions and a distillation loss on MLM and EMA (basically, we show a part of the image to the student and the whole image to the teacher, we do MLM with this too) rather than contrastive (so the first one is a classification task) map head for pooling [cite: 356]"
Swe rl grpo,"clone un gros paquet de repo avec PR, prend en compte le status avant le merge et après et les discussions, filtre les PR vides, avec trop de changements et [cite: 357]ajouté les files pas changés mais utiles pour la PR. [cite: 357]GRPO avec −1 sur le format et entre 0 et 1 par un séquence matcher sur le résultat officiel de la PR[cite: 358]. C'est mieux d'avoir un reward continu que discret (−1 ou 1). [cite: 359]Le RL garde bien le reasoning OOD[cite: 359]. Ça utilise agentless mini qui sépare les tâches en des tâches simples donc c'est limitant ça ça ne gère pas tout d'un coup [cite: 359]","clones a large number of repos with PRs, takes into account the status before and after the merge and discussions, filters empty PRs, PRs with too many changes, and [cite: 357]added files not changed but useful for the PR. [cite: 357]GRPO with −1 on the format and between 0 and 1 via a sequence matcher on the official PR result[cite: 358]. It is better to have a continuous reward than a discrete one (−1 or 1). [cite: 359]RL maintains good OOD reasoning[cite: 359]. It uses agentless mini which separates tasks into simple tasks, so this is limiting as it doesn't manage everything at once [cite: 359]"
Algorithme GRPO,À chaque itération principale on prend le modèle comme init[cite: 360]. Ensuite on fait un nombre de steps en fonction du nombre de batch. [cite: 360]Pour chaque batch le dernier modèle devient le old puis on compute le reward pour chaque output et l'advantage pour le batch. [cite: 361]Ensuite on fait des itérations de GRPO pour améliorer les paramètres et on change de batch (donc de old) sur tout le set (on peut faire plusieurs itérations en changeant de réf) [cite: 361],"At each main iteration we take the model as init[cite: 360]. Then we do a number of steps based on the number of batches. [cite: 360]For each batch, the last model becomes the old one, then we compute the reward for each output and the advantage for the batch. [cite: 361]Then we do GRPO iterations to improve the parameters and we change batch (and thus old model) on the whole set (we can do several iterations by changing ref) [cite: 361]"
Self gauche reasoner tools alibaba (STAR),quand QWQ utilise des tools python il est bien meilleur mais il ne suit pas l'instruction. [cite: 361] Donc ils ont fait STAR en 3 étapes. Insérer des hints d'utilisation de python sur les mots clés de réflexion et changement de direction. [cite: 362] Rejection sampling pour créer une première version avec les hints qui marchent bien. Rejection sampling sans hint pour faire STAR. [cite: 363]Grosse hausse de perf. [cite: 364],when QWQ uses Python tools it is much better but it doesn't follow instructions. [cite: 361] So they did STAR in 3 steps. Insert hints for Python usage on reflection and direction change keywords. [cite: 362] Rejection sampling to create a first version with hints that work well. Rejection sampling without hints to do STAR. [cite: 363]Big performance increase. [cite: 364]
Gemma 3,"multi modal, local et global 5:1, ablations sur ça, le scale du RoPE, la taille du teacher pour distiller (si plus de token gros c'est mieux) [cite: 364]","multi-modal, local and global 5:1, ablations on this, the RoPE scale, the teacher size for distillation (if more large tokens it's better) [cite: 364]"
Meta renforcement fine tuning (Cumulative Regret),l'idée est d'entraîner et de juger sur du cumulative regret. [cite: 365]En gros on découpe le truc en épisode et on check si cet épisode nous approche de la réponse que ce soit en exploitation ou exploration il y a un meta prover LLM qui établit une réponse à partir de ce qu'on a déjà fait comme épisode. [cite: 365]En plus on est capable d'analyser des LLM avec cette métrique et trouver que plus de budget et plus long COT ne fait pas forcément de meilleures réponses en full outcome reward. [cite: 366]En plus ici le reward est dense [cite: 367],"the idea is to train and judge on cumulative regret. [cite: 365]Basically, we cut the thing into episodes and check if this episode brings us closer to the answer, whether in exploitation or exploration, there is a meta prover LLM which establishes a response based on what has already been done as an episode. [cite: 365]In addition, we are able to analyze LLMs with this metric and find that more budget and longer COT do not necessarily lead to better answers with full outcome reward. [cite: 366]Moreover, here the reward is dense [cite: 367]"
Dapo,"GRPO sauf qu'on exclut la KL divergence, on met un clip high et low différent (pour les petites probas), on moyenne sur le nombre de token après la somme sur le batch pour pas pénaliser les longues réponses et on choisit que les réponses qui n'ont pas 0 ou 1 d'accuracy (dynamic sampling) [cite: 367]","GRPO except that KL divergence is excluded, different high and low clips are set (for small probabilities), we average over the number of tokens after summing over the batch so as not to penalize long responses, and only responses that don't have 0 or 1 accuracy are selected (dynamic sampling) [cite: 367]"
Why do multi agent fails,"taxonomie sur les fails des multi agents systèmes orientés autour de poor specification, inter agent miss alignment et task verification failure[cite: 367]. Pipeline LLM as a judge sur ça [cite: 368]","taxonomy on multi-agent system failures focused on poor specification, inter-agent misalignment, and task verification failure[cite: 367]. LLM as a judge pipeline on this [cite: 368]"
Qwen omni,"thinker (LLM) et talker (transformer audio), tmRoPE qui est un RoPE 3D, un pas de RoPE est un mot ou 40ms d'audio ou vidéo et height et width. [cite: 368]Si vidéo avec audio c'est 2 secondes d'audio puis une image. [cite: 369]Talker reçoit direct le token mais aussi l'embedding de sortie même de thinker pour prévoir mieux le ton et la suite mais dire le bon mot [cite: 370]","thinker (LLM) and talker (audio transformer), tmRoPE which is a 3D RoPE, one RoPE step is one word or 40ms of audio or video and height and width. [cite: 368]If video with audio, it's 2 seconds of audio then an image. [cite: 369]Talker receives the token directly but also the output embedding even from thinker to better predict the tone and the continuation but say the right word [cite: 370]"
Metamorphe vipt meta,prediction de token image sur espace continu qui est l'espace embedding du ViT encoder avec cosine loss avec une tête vision spécifique déclenchée par un token image begin. [cite: 370] Puis diffusion avec une cross attention sur les token image plutôt que text. Super perf en reason. [cite: 371]Finding que VLM généralise bien à la génération d'image [cite: 372],image token prediction on continuous space which is the embedding space of the ViT encoder with cosine loss with a specific vision head triggered by an image begin token. [cite: 370] Then diffusion with cross-attention on image tokens rather than text. Great performance in reasoning. [cite: 371]Finding that VLM generalizes well to image generation [cite: 372]
Transformer without normalization (Dyt),la LN dans la plupart des transformers fait un truc linéaire par token ou channel de l'embedding mais en regardant en globalité ça fait de la non linéarité comme un tanh avec un param. [cite: 372] Donc Dyt remplace par ça. Ne marche pas bien hors transformers [cite: 373],LN in most transformers does something linear per token or embedding channel but looking globally it creates non-linearity like a tanh with a parameter. [cite: 372] So Dyt replaces it with that. Doesn't work well outside of transformers [cite: 373]
Rope (Principes),"l'objectif est purement d'avoir une position relation ici cos(i−j). [cite: 373]Ça se passe au niveau de la dimension des embedding et c'est fait pour chaque token sur q et k donc que dans le softmax. [cite: 374]La value reste sur la même espace. [cite: 375]Ensuite plus la fréquence est grande, moins on tourne vite, moins on est sensible proche (on reste bien pour les longs contextes). [cite: 375]On a aussi la position dans l'embedding qui joue et permet de capturer dans les premières dimensions les contextes plus proches et les dimensions grosses les contextes plus longs [cite: 376]","the objective is purely to have a positional relation here cos(i−j). [cite: 373]This happens at the level of the embedding dimension and is done for each token on q and k, so only in the softmax. [cite: 374]The value remains in the same space. [cite: 375]Then the higher the frequency, the slower we rotate, the less sensitive we are nearby (it remains good for long contexts). [cite: 375]We also have the position in the embedding that plays a role and allows to capture closer contexts in the first dimensions and longer contexts in the large dimensions [cite: 376]"
Proof or bluff,"modèle testé sur olympiade math et avec des humains en juge. [cite: 376]Finalement pas très bons pour manque de logique, assomption, créativité et algèbre. [cite: 377]LLM as a judge tend à surévaluer les performances. [cite: 378]","model tested on math olympiad and with humans as judges. [cite: 376]Finally not very good due to lack of logic, assumptions, creativity, and algebra. [cite: 377]LLM as a judge tends to overestimate performance. [cite: 378]"
Ui r1,"GRPO avec un reward en 3 parties : action, coordonnées du clic et format de la réponse (car thinking) permet de faire un meilleur modèle qu'un 7b 1m de data avec 3b 126 data [cite: 378]","GRPO with a reward in 3 parts: action, click coordinates, and response format (because of thinking) makes it possible to create a better model than a 7b 1m data model with 3b 126 data [cite: 378]"
Papier Claude cot reasoning model dont always say what they think,les COT cachent leur reasoning souvent et font du reward hack [cite: 378],COT often hide their reasoning and perform reward hacking [cite: 378]
Command a,"3:1 SWA et full NOPE, GQA SwiGLU. [cite: 379]No bias, merging de models entraînés sur 6 scores en SFT et idem en RL (RAG tool agent, multilingual, code, math et reasoning, long context, safety, IF) avec des dataset spécifiques pour chaque. [cite: 379]RL preference avec SRPO qui en gros entraîne 2 policy pour maximiser le reward tout en réduisant le reward gap entre les 2. RL VR COPG qui ressemble à RLOO. [cite: 380]Reward du code sur le passage des test en %, Warp merging et linear merging pour le code. [cite: 381]","3:1 SWA and full NOPE, GQA SwiGLU. [cite: 379]No bias, merging of models trained on 6 scores in SFT and same in RL (RAG tool agent, multilingual, code, math and reasoning, long context, safety, IF) with specific datasets for each. [cite: 379] RL preference with SRPO which basically trains 2 policies to maximize the reward while reducing the reward gap between the two. RL VR COPG which resembles RLOO. [cite: 380]Code reward on test passing in %, Warp merging and linear merging for code. [cite: 381]"
Deepseek généraliste reward modeling (GRM),"on apprend au GRM à générer des principes puis il score avec ça, c'est en self sur du reward et du sampling pour les principes. [cite: 382] Faire avec des principes filtrés ça marche très bien. Le papier explique aussi les différents types de rewards [cite: 383]",we teach the GRM to generate principles then it scores with them; this is done in self-supervision on reward and sampling for the principles. [cite: 382] Doing it with filtered principles works very well. The paper also explains the different types of rewards [cite: 383]
Synthétique data generation and multistep reasoning for tool Use (SWIRL),"Google, SWIRL step wise reinforcement learning. [cite: 383]On génère avec un LLM sous un format spécifique une action et un env réponse par step qui appelle et reçoit la réponse d'un outil. [cite: 384] A la fin on a la réponse. On filtre avec judge sur le process et outcome filtering. [cite: 385] Puis on s'en sert pour train. A l'inférence, on prompt et on répond à chaque appel de tools ce qui augmente de 10 à 20% sur les benchmarks un Gemma 27b [cite: 386]","Google, SWIRL step wise reinforcement learning. [cite: 383]We generate with an LLM under a specific format an action and an environment response per step that calls and receives the response from a tool. [cite: 384] At the end we have the answer. Filtering is done with a judge on the process and outcome filtering. [cite: 385] Then we use it for training. During inference, we prompt and respond to each tool call, which increases performance by 10 to 20% on benchmarks for a Gemma 27b [cite: 386]"
Retool,Chain of Thought avec code interprété dans la chaîne appris en RL avec un peu de SFT pour les token code et interpréter et du outcome reward. [cite: 386] PPO avec interpréteur caché. KV cache optimisé pour ne pas refaire tout le calcul. [cite: 387]Mixture of Block attention [cite: 388],Chain of Thought with code interpreted in the chain learned in RL with some SFT for the code and interpreter tokens and outcome reward. [cite: 386] PPO with hidden interpreter. Optimized KV cache to avoid redoing all the calculation. [cite: 387]Mixture of Block attention [cite: 388]
Phi 4 multimodal,"tied embedding, RoPE sur 75% des heads pour long contexte, LoRA pour les 2 modalités sur les couches linéaires [cite: 388]","tied embedding, RoPE on 75% of the heads for long context, LoRA for the 2 modalities on the linear layers [cite: 388]"
Tied embedidng,"en fait la matrice d'embedding qui est un lookup et la matrice de logit ou d'embedding qui est une couche dense sont de même dimensions mais on peut partager cette matrice en transposant car la couche dense est donc juste un dot product pour trouver le token le plus proche en distance (puis softmax). [cite: 389]Ça permet de réduire le nombre de paramètres, de faire de la régularisation (car l'input est lié à l'output) [cite: 389]","in fact the embedding matrix which is a lookup and the logit or embedding matrix which is a dense layer are of the same dimensions but this matrix can be shared by transposing because the dense layer is just a dot product to find the closest token in distance (then softmax). [cite: 389]This allows reducing the number of parameters, and regularization (because the input is linked to the output) [cite: 389]"
Deepseek prover,"COT avec V3 pour avoir les grandes parties d'une preuve, utilisation d'un prover 7b pour faire la preuve de chaque partie. [cite: 390]Training en SFT puis GRPO avec un mode preuve poussé ou un mode COT high level [cite: 390]","COT with V3 to get the main parts of a proof, using a 7b prover to make the proof for each part. [cite: 390]Training in SFT then GRPO with an advanced proof mode or a high-level COT mode [cite: 390]"
Prm that think,objectif reasoning pour vérifier les steps et boxed avec correct et incorrecte. [cite: 390] Utilise de la donnée synthétique sur PRM800k mais sur 1k. Générée avec QWQ 32b. [cite: 391] LLM as a judge en reasoning tend à overthink et essayer de retrouver. Train empêche l'overthink et améliore les résultats. [cite: 392] LoRA 32b sur une A100 pendant 4 heures. Batch size 16 [cite: 393],reasoning objective to verify steps and boxed with correct and incorrect. [cite: 390] Uses synthetic data on PRM800k but only 1k. Generated with QWQ 32b. [cite: 391] LLM as a judge in reasoning tends to overthink and try to backtrack. Training prevents overthinking and improves results. [cite: 392] LoRA 32b on an A100 for 4 hours. Batch size 16 [cite: 393]
1 shot rlvr,RLVR sur un exemple très complexe n'overfit pas sur le test set [cite: 393],RLVR on a very complex example does not overfit on the test set [cite: 393]
Prime intellect paper,faire l'inférence sur des serveurs partagés et l'update sur un serveur sûr. [cite: 393]Update les poids et partager les data efficacement[cite: 394]. Toploc vérifie l'intégrité des inférences grâce à un sha256 des checkpoints[cite: 394]. Accepte les générations jusqu'à 5 checkpoints précédents [cite: 394],perform inference on shared servers and update on a secure server. [cite: 393]Update weights and share data efficiently[cite: 394]. Toploc verifies inference integrity thanks to a SHA256 of the checkpoints[cite: 394]. Accepts generations up to 5 previous checkpoints [cite: 394]
Qwen 3 tech report,"archi classique, 3 stages de pretrain avec normal, high qualité en montant le LR, long context[cite: 394]. Thinking et non thinking mode +[cite: 395]. Post Training : COT facile mais besoin de COT filtré avec 2.5 sample avec QWQ, long RL qui a besoin de thinking 4000 paires avec GRPO, non thinking, alignement[cite: 395]. Possibilité de stopper la génération mid thinking[cite: 395]. Distillation des tokens puis des logits [cite: 395]","classic architecture, 3 pretrain stages with normal, high quality by increasing the LR, long context[cite: 394]. Thinking and non-thinking mode +[cite: 395]. Post Training: easy COT but need filtered COT with 2.5 sample with QWQ, long RL that needs thinking 4000 pairs with GRPO, non-thinking, alignment[cite: 395]. Possibility to stop generation mid thinking[cite: 395]. Distillation of tokens then logits [cite: 395]"
Alpha évolve,"en gros un humain propose un problème, une première solution et une évaluation automatique et un LLM fait évoluer le code grâce à l'historique pour améliorer les résultats sur l'évaluation. [cite: 395]","essentially a human proposes a problem, a first solution and an automatic evaluation, and an LLM evolves the code thanks to the history to improve the results on the evaluation. [cite: 395]"
Byte dance pretrain merging,"avec model merging tous les V tokens et pour N models avec une pure moyenne permet de simplifier le scheduler avec un WSD, de stabiliser le training et de mieux le préparer pour les étapes d'après. [cite: 397]Avoir un plus gros LR permet de diminuer le coût [cite: 397]","with model merging all V tokens and for N models with a pure average allows to simplify the scheduler with a WSD, stabilize training, and better prepare it for subsequent stages. [cite: 397]Having a larger LR allows to reduce the cost [cite: 397]"
Rl finetunes small subnetworks in llm,le RL update les poids de façon sparse autour de 80% là où le SFT fait ça dense. [cite: 397]Contrairement à LoRA c'est en full rank et sur toutes les couches. [cite: 398]C'est le cas principalement car les data de RL sont in distribution [cite: 399],"RL updates weights sparsely around 80% while SFT does it densely. [cite: 397]Unlike LoRA, it's full rank and on all layers. [cite: 398]This is mainly because RL data is in distribution [cite: 399]"
Harnessing the universal geometry of embeddings,en apprenant un latent space (avec un generator qui a pour but de translate dans un espace latent) et un discriminator qui force à garder la distribution originale. [cite: 399]En fait dans le latent tous les embeddings sont très similaires et donc c'est facile de retrouver de quoi on parle [cite: 400],"by learning a latent space (with a generator whose goal is to translate into a latent space) and a discriminator which forces the original distribution to be kept. [cite: 399]In fact, in the latent space all embeddings are very similar and so it's easy to find what we're talking about [cite: 400]"
Prolonged rl expands reasoning,"GRPO orienté long train avec Dapo, high temp pour entropy et KL penalty car on part d'un algo déjà COT. [cite: 401]Mais ils ajoutent un reset pour repartir d'une snapshot plus récente sur la KL et prolonger le train quand le modèle stagne. [cite: 401] Ça scale hyper bien en OOD. Le RL apprend vraiment des trucs nouveaux au modèle surtout quand on entraîne longtemps [cite: 402]","GRPO oriented for long training with Dapo, high temp for entropy and KL penalty because we start from an already COT algorithm. [cite: 401]But they add a reset to restart from a more recent KL snapshot and prolong training when the model stagnates. [cite: 401] It scales very well in OOD. RL really teaches the model new things, especially when training for a long time [cite: 402]"
Entropy minimization,objectif est de réduire l'entropie donc maximiser la confiance sur un exemple. [cite: 402]En peu de steps et un exemple on obtient un modèle plus confiant qui s'avère meilleur [cite: 403],"objective is to reduce entropy, thus maximizing confidence on an example. [cite: 402]In few steps and one example we get a more confident model which turns out to be better [cite: 403]"
Reward anything,modèle de reward basé sur des principes (prompt) avec un GRPO sur format et accuracy pour le modèle de reward. [cite: 403]Le modèle rank les réponses et les score [cite: 404],reward model based on principles (prompt) with a GRPO on format and accuracy for the reward model. [cite: 403]The model ranks the responses and scores them [cite: 404]
Deepresearch bench,"100 tâches PhD level représentatives. [cite: 404]RACE : poids décidé par LLM as judge sur 4 critères : comprehensive, depth, instructions following, readbility. [cite: 405]Puis on donne au juge une référence top niveau sur laquelle il score (on ground par le haut)[cite: 406]. FACT : check les citations, regarde si la réponse est ground et combien de citations pertinentes sont retrouvées [cite: 406]","100 representative PhD level tasks. [cite: 404]RACE: weight decided by LLM as judge on 4 criteria: comprehensive, depth, instructions following, readability. [cite: 405]Then the judge is given a top-level reference on which it scores (grounding from the top)[cite: 406]. FACT: checks citations, looks at whether the response is grounded and how many relevant citations are found [cite: 406]"
How visual repr3sentations map to language feature space in multimodal llms,SAE sur Gemma 2 avec la layer de projection train et le ViT et LLM freeze. [cite: 406]Les features de visions convergent seulement dans les dernières layers ce qui peut gâcher le potentiel et pose des questions d'alignement [cite: 407],SAE on Gemma 2 with the trained projection layer and the ViT and LLM frozen. [cite: 406]Vision features only converge in the last layers which can spoil the potential and raises alignment issues [cite: 407]
Bytedance dolphin,"encoder décoder pour parser les documents images, texte, formules, tables. [cite: 407]Swin transformers et MBART fine tune sur 30m doc en anglais et chinois. [cite: 408]2 passes : une qui détecte dans l'ordre de lecture le layout et les éléments avec leur bounding box avec un prompt. [cite: 409]Une qui prend la photo de l'élément avec un prompt spécifique à l'élément pour le parser et le remettre en markdown. [cite: 410]320M et très performant. [cite: 411]","encoder-decoder to parse image documents, text, formulas, tables. [cite: 407]Swin transformers and MBART fine-tuned on 30m docs in English and Chinese. [cite: 408]2 passes: one that detects the layout and elements with their bounding box in reading order with a prompt. [cite: 409]One that takes a picture of the element with a specific prompt for the element to parse it and put it back in markdown. [cite: 410]320M and very performant. [cite: 411]"
Magistral,"GRPO custom (pas de KL donc pas de réf, upper et lower différentes, normalisation du batch et des advantages) pas de truc full juste ou full faux (ressemble à Dapo). [cite: 411]Reward format (code math think) correct length et language (problème, thought et answer dans la même langue) généralise bien aux autres langues[cite: 412]. RL only marche très bien pas forcément besoin de distiller[cite: 412]. Entraîner sur du text améliore les perfs multimodales[cite: 412]. Asynchrone RL : les générations sont continues et les poids des modèles updates en pleine séquence[cite: 412]. Augmente la difficulté, la complétion length et diminue le batch au fur et à mesure du RL [cite: 412]","custom GRPO (no KL so no ref, different upper and lower, normalization of batch and advantages) no full correct or full wrong things (resembles Dapo). [cite: 411]Reward format (code math think) correct length and language (problem, thought, and answer in the same language) generalizes well to other languages[cite: 412]. RL only works very well, not necessarily needing to distill[cite: 412]. Training on text improves multimodal performance[cite: 412]. Asynchronous RL: generations are continuous and model weights update mid-sequence[cite: 412]. Increases difficulty, completion length, and decreases batch size as RL progresses [cite: 412]"
Llm get lost in multiturn conversation,49% de average drop en perf entre simple et multi [cite: 413],49% average drop in performance between single-turn and multi-turn [cite: 413]
Gemma 3n,"audio, image, text. [cite: 413]Matryoshka pour réduire les params, per layer embedding caching pour améliorer la performance des layers. [cite: 413]Possible de offload les paramètres multimodaux. [cite: 414]","audio, image, text. [cite: 413]Matryoshka for parameter reduction, per layer embedding caching to improve layer performance. [cite: 413]Possible to offload multimodal parameters. [cite: 414]"
Hunyuan,"classique SwiGLU MoE 80b 14b active. [cite: 414]Pré Training sur 20T tokens avec annealing et long context avec NTK. [cite: 415]Post Training en dual mode (token thinking empty) avec SFT reasoning, RL reasoning et SFT RL full très divers sur base de GRM sur understanding, long context, creative, agents et MCP, multi-turn, instructions, rôle, safety, QA, multilingual, finance. [cite: 416]Bonnes perfs mais apparemment très benchmarké [cite: 417]","classic SwiGLU MoE 80b, 14b active. [cite: 414]Pre-training on 20T tokens with annealing and long context with NTK. [cite: 415]Post-training in dual mode (thinking token empty) with SFT reasoning, RL reasoning and very diverse full SFT RL based on GRM on understanding, long context, creative, agents and MCP, multi-turn, instructions, role, safety, QA, multilingual, finance. [cite: 416]Good performance but apparently heavily benchmarked [cite: 417]"
Jina v4,"J VDR benchmark avec pas que des questions et du text. [cite: 417]Train sur du retrieval mais aussi sémantique text similarity pour screenshots. [cite: 418] Train sur dense et late interaction et text et multimodal. LoRA pour retriever, code et symétrique. [cite: 419] Analyse des embeddings unifiés en multimodal. Qwen 2.5 VL based [cite: 420]","J VDR benchmark not only with questions and text. [cite: 417]Trained on retrieval but also semantic text similarity for screenshots. [cite: 418] Trained on dense and late interaction and text and multimodal. LoRA for retriever, code and symmetric. [cite: 419] Analysis of unified multimodal embeddings. Qwen 2.5 VL based [cite: 420]"
Mrr / Ndcg,MRR : quand on s'inquiète seulement du rang du premier relevant[cite: 420]. NDCG : quand on s'inquiète du rang et ordre de tous les relevant [cite: 421],MRR: when we only care about the rank of the first relevant item[cite: 420]. NDCG: when we care about the rank and order of all relevant items [cite: 421]
Websailor,"construction de data synthetic et training de modèle Deepresearch. [cite: 421]Deepresearch marche que en propriétaire, les open source marchent que pour des simples recherches ou des trucs où le chemin de résolution est simple. [cite: 422]Il manque le level 3 qui est d'avoir une généralisation quand les tâches n'ont pas de solution claire et définie. [cite: 423]Ils ont pris des graphes de parcours de pages sur des entités complexes pour en extraire des tâches complexes (jusqu'à 40 calls nécessaires pour O3) et utilise des LRM pour extraire des étapes concises de thinking pour faire un dataset. [cite: 424] Phase de rejection sampling SFT puis RL. ReAct et rejection sampling et DUPO : comme ils veulent faire comme Dapo et filtrer les exemples avec full ou 0 reward mais que le rollout coûte cher, ils dupliquent certains reward pour combler le batch [cite: 425]","construction of synthetic data and Deepresearch model training. [cite: 421]Deepresearch only works in proprietary models, open source models only work for simple searches or things where the resolution path is simple. [cite: 422]Level 3 is missing, which is to have generalization when tasks do not have a clear and defined solution. [cite: 423]They topage traversal graphs on complex entities to extract complex tasks (up to 40 calls needed for O3) and use LRM to extract concise thinking steps to create a dataset. [cite: 424] Rejection sampling SFT phase then RL. ReAct and rejection sampling and DUPO: since they want to do like Dapo and filter examples with full or 0 reward but rollout is expensive, they duplicate some rewards to fill the batch [cite: 425]"
Voxtral,Whisper v3 encoder sur des chunks de 30sec en bidirectionnel qui sont concat sans attention commune. [cite: 425]MLP layer pour downsample de 4 fois ce qui fait 12.5kHz donc 32k pour 40 minutes. [cite: 426] Plug à Ministral ou Mistral small. Pretraining sur transcription et audio puis text complétion pour aligner les modes. [cite: 427] Avec des tokens de contrôle. Puis SFT sur des tâches audio et texte [cite: 428],Whisper v3 encoder on 30sec bidirectional chunks that are concatenated without common attention. [cite: 425]MLP layer to downsample 4 times which makes 12.5kHz so 32k for 40 minutes. [cite: 426] Plugs into Ministral or Mistral small. Pretraining on transcription and audio then text completion to align modes. [cite: 427] With control tokens. Then SFT on audio and text tasks [cite: 428]
Nemo Retriever colembed,"Nvidia Eagles fine tune en 2 stages sur text puis image en late interaction. [cite: 428] Découpage de l'image en petit et thumbnails. Finetune pour passer de causal à bidirectionnel. [cite: 429]Dans la InfoNCE la loss est avec 2 négatives qui sont le top2 sans ceux au-dessus de 0.95 de score. [cite: 430] Attention au tiling, max token et embedding dim. Ablations sur le stockage, l'embedding, le reranker et bi encoder. [cite: 431]","Nvidia Eagles fine-tuned in 2 stages on text then image in late interaction. [cite: 428] Image slicing into small pieces and thumbnails. Finetune to switch from causal to bidirectional. [cite: 429]In InfoNCE, the loss is with 2 negatives which are the top 2 without those above 0.95 score. [cite: 430] Attention to tiling, max token, and embedding dim. Ablations on storage, embedding, reranker, and bi encoder. [cite: 431]"
Kimi k2 paper,"MoE 1T 32B active, beaucoup de token MuonCLIP at scale (Adaptive CLIP sur k et q). [cite: 432] Training méga stable. Pipeline de génération synthétique de data agentique comme Acebench avec des milliers de tools en env. [cite: 433]Un LLM as a judge extrait les données de haute qualité pour le train. [cite: 434]Général RL system avec Self judge pour les non vérifiable rewards. [cite: 435]La première couche est un dense[cite: 436]. L'algo est un GRPO pur RL sans advantage × avec la moyenne juste et la KL est dans la somme ? [cite: 436] Ou juste le ratio. Pénalité si réponse trop longue et loss PTX qui vérifie que le modèle overfit pas sur le reward. [cite: 437] Température decay pendant le Training. D'exploration à exécution [cite: 438]","MoE 1T 32B active, lots of MuonCLIP at scale tokens (Adaptive CLIP on k and q). [cite: 432] Mega stable training. Synthetic agentic data generation pipeline like Acebench with thousands of tools in the environment. [cite: 433]An LLM as a judge extracts high-quality data for training. [cite: 434]General RL system with Self judge for non-verifiable rewards. [cite: 435]The first layer is a dense one[cite: 436]. The algorithm is a pure RL GRPO without advantage × with just the average and the KL is in the sum? [cite: 436] Or just the ratio. Penalty if the response is too long and PTX loss that verifies the model doesn't overfit on the reward. [cite: 437] Temperature decay during training. From exploration to execution [cite: 438]"
Muon,"Niveau matrice de full param et gradient, pas element-wise. [cite: 438]Momentum, pas de vélocité donc premier ordre et pas seconde ordre. [cite: 439]Par contre ça utilise le full paramètres dans l'inverse donc le preconditioning est beaucoup plus riche sur les dimensions[cite: 440]. Newton Schultz approxime le −1/2 (inverse squared) qui est le classique precond en optim (sauf que c'est normalement celui du hessien mais coûte en calcul)[cite: 440]. C'est que pour du 2D en terme de param donc la couche d'embedding reste avec du Adam [cite: 440]","Level of full parameter and gradient matrix, not element-wise. [cite: 438]Momentum, no velocity, so first order and not second order. [cite: 439]However, it uses the full parameters in the inverse, so preconditioning is much richer on the dimensions[cite: 440]. Newton Schultz approximates −1/2 (inverse squared) which is the classic precond in optim (except it's normally the Hessian's but is expensive to calculate)[cite: 440]. It's only for 2D in terms of parameters, so the embedding layer remains with Adam [cite: 440]"
Absence benche,NIAH mais version omission. [cite: 441]On propose un document et une version avec une partie supprimée et demande de retrouver ce qui a été omis. [cite: 441]Pour l'instant les perfs sont mauvaises alors que les contextes sont petits. [cite: 442]Un peu un benchmark sur la comparaison [cite: 443],"NIAH but omission version. [cite: 441]A document is proposed and a version with a removed part and it is asked to find what was omitted. [cite: 441]For now, performance is poor even though contexts are small. [cite: 442]A bit of a benchmark on comparison [cite: 443]"
From bytes to ideas,"U-Net sur les bytes pour se priver de tokenizer, stade de recherche. [cite: 443]Un peu comme le meta no token. [cite: 444]","U-Net on bytes to avoid using a tokenizer, research stage. [cite: 443]A bit like the meta no token. [cite: 444]"
Dynamic chunking,"similarité avec bytes to idea, l'idée est d'avoir un autoencoder H net qui chunk dynamiquement les bytes à partir d'une limite d'entropie et similarité puis fournit ça à un main network (genre un LLM) les chunks sont ensuite décodés en bytes [cite: 444]","similarity with bytes to idea, the idea is to have an autoencoder H net that dynamically chunks the bytes from an entropy and similarity limit then provides it to a main network (like an LLM) the chunks are then decoded into bytes [cite: 444]"
Gspo qwen,"GRPO et PPO utilisent le clip pour ne pas trop s'éloigner du old (c'est le proximal) mais en fait le reward est au niveau de la phrase et GRPO au niveau du token donc un haut risque [cite: 445] de variance sur le long contexte. GSPO traite le clip et le ratio au niveau de la séquence et normalise ça, ce qui corrige le problème. [cite: 445]Ça clip beaucoup plus mais améliore la stabilité et les résultats [cite: 446]","GRPO and PPO use clipping so as not to move too far from the old policy (it's the proximal part) but in fact the reward is at the phrase level and GRPO at the token level so a high risk [cite: 445] of variance on long context. GSPO treats clipping and the ratio at the sequence level and normalizes it, which corrects the problem. [cite: 445]It clips much more but improves stability and results [cite: 446]"
Webshaper,"système formal de génération de dataset pour information seeking (IS). [cite: 447]L'idée est de créer un dataset de base à partir de Wikipedia sur un graphe d'entité puis de complexifier les questions avec un agent expander. [cite: 447]Le système formalisé permet d'éviter les hallucinations et structurer les graphes. [cite: 448]L'agent expander a des tools de recherche, résumé et validation. [cite: 449]","formal system for dataset generation for information seeking (IS). [cite: 447]The idea is to create a base dataset from Wikipedia on an entity graph then to complexify the questions with an expander agent. [cite: 447]The formalized system allows avoiding hallucinations and structuring the graphs. [cite: 448]The expander agent has search, summary, and validation tools. [cite: 449]"
Attention sinks,sliding window perd le contexte initial. [cite: 449]L'idée est de toujours garder le KV des premiers tokens (qui ont beaucoup d'importance) et de faire sliding window sur le reste. [cite: 450],sliding window loses the initial context. [cite: 449]The idea is to always keep the KV of the first tokens (which are very important) and to use sliding window on the rest. [cite: 450]
Small language model are the future of agentic,"défense du SLM contre LLM pour l'agentique : les SLM sont puissants, peu chers, fine-tunables et améliorables. [cite: 451]Les agents sont très fermés, précis et utilisent peu de choses du LLM. [cite: 452]Pour ça il faut des benchmark et une bonne évaluation. [cite: 453]","defense of SLM against LLM for agentic tasks: SLMs are powerful, inexpensive, fine-tunable, and improvable. [cite: 451]Agents are very closed, precise, and use little of the LLM. [cite: 452]For this, benchmarks and good evaluation are needed. [cite: 453]"
Seed prover,seed geometry train sur tonggeometry pour les tâches géométriques (ils ont test actor critic mais un seul modèle est mieux). [cite: 454]Seed prover qui utilise Lean 4 en mode lemma qui sont vérifiés pour suivre ce qui marche ou pas et ce qui compile puis theorem. [cite: 455]3 modes : Light qui prove en boucle en améliorant sur le feedback du compiler. [cite: 456]Médium qui refine les fails en ajoutant les pass dans le context. [cite: 457]Heavy qui génère une multitude de facts avec le Light puis les utilise pour le Médium avec un LLM as judge. [cite: 458],"seed geometry trained on tonggeometry for geometric tasks (they tested actor critic but a single model is better). [cite: 454]Seed prover that uses Lean 4 in lemma mode which are verified to track what works or not and what compiles then theorem. [cite: 455]3 modes: Light which proves in a loop, improving based on compiler feedback. [cite: 456]Medium which refines failures by adding passes to the context. [cite: 457]Heavy which generates a multitude of facts with Light then uses them for Medium with an LLM as judge. [cite: 458]"
Gepa,optimisation de prompts par Pareto. L'idée est de générer des candidats d'amélioration de prompts et de les merge ou fusionner si ça améliore une métrique. [cite: 459]Les prompts sont aussi générés avec les traces de la métrique (le détail du score et les steps). [cite: 460] Ça marche vraiment bien. Le Pareto sert à ne pas se focus sur un local. [cite: 461],prompt optimization by Pareto. The idea is to generate candidates for prompt improvement and merge or fuse them if it improves a metric. [cite: 459]Prompts are also generated with the traces of the metric (the detail of the score and the steps). [cite: 460] It works really well. Pareto helps to avoid focusing on a local optimum. [cite: 461]
Hiérarchical reasoning model,hiérarchie avec low level qui coûte pas cher et fait plein de steps et high level qui fait une grosse step pour guider le low level. [cite: 462] RNN. Approximation des gradients sans stocker toutes les forwards mais seul les derniers par rapport à l'input. [cite: 463],hierarchy with low level which is cheap and takes many steps and high level which takes a big step to guide the low level. [cite: 462] RNN. Gradient approximation without storing all forwards but only the last ones relative to the input. [cite: 463]
Qwen image tech report,"MMDIT qui prend pour input Qwen VL pour l'alignement sémantique/image et un VAE encoder pour la reconstruction fiable d'images. [cite: 464] 2 tâches principales t2i et ti2i. MSRoPE qui au lieu de faire comme 2D RoPE et de mettre le text en (x,0) fait sur la diag (x,x) avec l'image centrée en 0. [cite: 465]Grosse étape de filtering des images en 7 stages et en augmentant la qualité. [cite: 466]Train sur la vélocité dans l'espace latent (vélocité = différence entre cst​×noise et cst2​×image de base - le training se fait en samplant un timestep et en prédisant bien la vélocité) [cite: 466]","MMDIT which takes Qwen VL as input for semantic/image alignment and a VAE encoder for reliable image reconstruction. [cite: 464] 2 main tasks t2i and ti2i. MSRoPE which instead of doing like 2D RoPE and putting the text at (x,0) does it on the diagonal (x,x) with the image centered at 0. [cite: 465]Big 7-stage image filtering step and quality improvement. [cite: 466]Train on velocity in latent space (velocity = difference between cst​×noise and cst2​×base image - training is done by sampling a timestep and correctly predicting the velocity) [cite: 466]"
Glm rl Training framework,sensiblement comme Magistral en asynchrone [cite: 467],essentially like Magistral asynchronously [cite: 467]
Glm 4.5,"GLM 4.5 : thinking et non thinking unifié, mid training avec plusieurs epochs sur les data de qualité, post training avec SFT et RL unifié (general DPO, agentic, tool, IF, …) et difficulté qui augmente au cours du training + usage d'experts et self distillation (SFT, RL, SFT avec rejection sampling, RL). [cite: 467]RL Framework synchrone et async avec Ray, Megatron et SGland (slime) [cite: 467]","GLM 4.5: unified thinking and non-thinking, mid-training with multiple epochs on quality data, post-training with unified SFT and RL (general DPO, agentic, tool, IF, …) and increasing difficulty during training + use of experts and self-distillation (SFT, RL, SFT with rejection sampling, RL). [cite: 467]Synchronous and async RL Framework with Ray, Megatron, and SGland (slime) [cite: 467]"
Gpt oss,"attention sink, sliding, attention is off by one et classique [cite: 468]","attention sink, sliding, attention is off by one and classic [cite: 468]"
Glm (RL),"adapte la difficulté au fur et à mesure. [cite: 468] Mix RLVR et RLHF. Clip plus haut et pas de KL divergence pour les modèles de vision. [cite: 468]SFT : token level loss donc backprop et gradient au niveau du token puis average[cite: 469]. GRPO et GSPO : c'est plutôt au niveau de la séquence mais comme on a des rollouts et un advantage, on sait quel rollout impacte mieux et vers qui on doit aller niveau séquence [cite: 469]","gradually adapts difficulty. [cite: 468] Mix RLVR and RLHF. Higher clip and no KL divergence for vision models. [cite: 468]SFT: token level loss so backprop and gradient at the token level then average[cite: 469]. GRPO and GSPO: it's more at the sequence level but since we have rollouts and an advantage, we know which rollout impacts better and where we should go at the sequence level [cite: 469]"
Byte dance multimodal agent long terme memory,"un modèle qui construit une mémoire à partir de vidéos et audio en stockant les images, audio et des sémantiques et metadata dans un graphe par entité. [cite: 470]Ensuite modèle train sur Dapo pour RAG et réponse à partir de la mémoire [cite: 470]","a model that builds memory from videos and audio by storing images, audio and semantics and metadata in a graph per entity. [cite: 470]Then model trained on Dapo for RAG and response from memory [cite: 470]"
Treat paper diffusion,diffusion en skippant des layers par token améliore les performances [cite: 471],diffusion by skipping layers per token improves performance [cite: 471]
Computer rl,génération d'API à partir de GUI pour Training et unification API/GUI. [cite: 471]Alternance entre RL et SFT pour limiter l'entropie collapse en réutilisant des trajectoires qui ont fonctionné. [cite: 472]Step wise GRPO [cite: 473],API generation from GUI for training and API/GUI unification. [cite: 471]Alternation between RL and SFT to limit entropy collapse by reusing trajectories that worked. [cite: 472]Step wise GRPO [cite: 473]
Real MM rag ibm,filtering de questions par LLM et rephrasing pour pas avoir les mots de la page direct. [cite: 473] Accurate labelling en faisant toutes les permutations par page avec un VLM. Cher mais utile. [cite: 474],question filtering by LLM and rephrasing so as not to have the words of the page directly. [cite: 473] Accurate labeling by doing all permutations per page with a VLM. Expensive but useful. [cite: 474]
Dupo bytedance,RLHF et RLVR coûtent cher[cite: 475]. L'idée est d'utiliser une dual task donc reconstruire l'input et train sur la distance de reconstruction. [cite: 476]Sauf que parfois c'est dur donc l'idée est de donner des éléments. [cite: 476]Exemple a+b=c on reconstruit a avec b et c. [cite: 477],RLHF and RLVR are expensive[cite: 475]. The idea is to use a dual task so reconstruct the input and train on the reconstruction distance. [cite: 476]Except that sometimes it's difficult so the idea is to provide elements. [cite: 476]Example a+b=c we reconstruct a with b and c. [cite: 477]
Bytedance pass@k Training,l'idée est se faire RLVR sur pass@k au lieu de pass@1. [cite: 478]Sauf que n rollout sur pass@k coûte cher donc on bootstrap (sample k random parmi les rollout le nombre de fois nécessaire) pour limiter les rollouts. [cite: 479] En fait on peut même avoir direct l'advantage d'une réponse positive et négative. Pass@k permet de mieux balancer l'exploration. [cite: 480],"the idea is to do RLVR on pass@k instead of pass@1. [cite: 478]Except that n rollout on pass@k is expensive so we bootstrap (sample k random among the rollouts the necessary number of times) to limit rollouts. [cite: 479] In fact, we can even directly have the advantage of a positive and negative response. Pass@k allows for better exploration balance. [cite: 480]"
Constitutional ai anthropic,l'idée est de self improve un modèle à partir de principes décrits par l'homme qui sont utilisés par le même modèle pour améliorer ou scorer des réponses plutôt que directement l'humain qui corrige ou note les réponses. [cite: 481]Chain of Thought améliore le process et il y a une step révision SFT et une step scoring preference et RLAIF [cite: 482],the idea is to self-improve a model based on human-described principles that are used by the same model to improve or score responses rather than directly by the human who corrects or rates the responses. [cite: 481]Chain of Thought improves the process and there is an SFT revision step and a preference scoring step and RLAIF [cite: 482]
Long cat llm,shortcut MoE avec 2 MLA et des FFN et MoE. [cite: 482]Le MoE shortcut sans passer par la 2ème MLA cela permet de passer la partie shortcut pendant le dispatch des EP sans perdre de temps de compute. [cite: 483]Zéro computation experts avec des experts qui retournent l'input directement. [cite: 484] Rescaling de q et k hors de la partie RoPE sur la dimension full pour améliorer la loss. Multi-token prediction. [cite: 485] Transfert d'hyperparamètres à partir d'un proxy. Stack d'un proxy un peu train pour l'initialisation. [cite: 486],shortcut MoE with 2 MLA and FFN and MoE. [cite: 482]MoE shortcut without passing through the 2nd MLA allows skipping the shortcut part during EP dispatch without losing compute time. [cite: 483]Zero computation experts with experts that return the input directly. [cite: 484] Rescaling of q and k outside of the RoPE part on the full dimension to improve the loss. Multi-token prediction. [cite: 485] Hyperparameter transfer from a proxy. Stacking of a slightly trained proxy for initialization. [cite: 486]
Sfr deep research,"single agent deep research avec pur RL sur un modèle optimisé pour reasoning. [cite: 487]Une partie du context est dédié à la mémoire et un tool pour clean est utilisé par l'agent lorsque ça dépasse. [cite: 488]3 tools très simples : search, browse page section et code interpreter. [cite: 489]Tous les token de thinking sont supprimés de l'historique chaque turn. [cite: 490]Notification d'erreur pour laisser retry ou reformuler et être fault tolerant. [cite: 491]Custom reinforce sur les steps avec une normalisation de l'advantage sur la length. [cite: 492]Ils disent que mettre tout sur les mêmes GPUs améliore l'utilisation contrairement à la séparation. [cite: 493]","single agent deep research with pure RL on a model optimized for reasoning. [cite: 487]Part of the context is dedicated to memory and a cleaning tool is used by the agent when it exceeds the limit. [cite: 488]3 very simple tools: search, browse page section, and code interpreter. [cite: 489]All thinking tokens are deleted from the history each turn. [cite: 490]Error notification to allow retry or reformulation and be fault tolerant. [cite: 491]Custom reinforce on the steps with a normalization of the advantage over the length. [cite: 492]They say that putting everything on the same GPUs improves utilization unlike separation. [cite: 493]"
Revolutionizing rl for diffusion,"TraceRL la policy est le diffuseur, le state la mask séquence et prompt et la prediction est la génération des token mask. [cite: 494]Recurrence frame based action [cite: 494]","TraceRL the policy is the diffuser, the state the mask sequence and prompt and the prediction is the generation of mask tokens. [cite: 494]Recurrence frame based action [cite: 494]"
Tiny récurrent models (HRM),"explication super claire de HRM avec les points qui posent problème. [cite: 495]Ils améliorent le modèle en n'ayant qu'un seul modèle au lieu de 2, ils font des ablations sur tous les paramètres (nombre de layers, nombre de fréquence) et choisissent 2 embeddings, un qui représente le latent et un qui représente la réponse. [cite: 496]Un forward correspond à n améliorations récursives du latent et une amélioration de la réponse. [cite: 497]Pendant T−1 étapes récursives on améliore la réponse puis on fait une dernière étape de deep supervision (comprendre backpropagation) pour faire une dernière passe. [cite: 498]Il y a une alternative sans couche d'attention avec un pur MLP pour les tâches à petits contextes à taille fixe [cite: 499]","super clear explanation of HRM with the problematic points. [cite: 495]They improve the model by having only one model instead of 2, they do ablations on all parameters (number of layers, number of frequency) and choose 2 embeddings, one representing the latent and one representing the answer. [cite: 496]A forward corresponds to n recursive latent improvements and one response improvement. [cite: 497]During T−1 recursive steps we improve the response then we do a last deep supervision step (meaning backpropagation) for a final pass. [cite: 498]There is an alternative without an attention layer with a pure MLP for small, fixed-size context tasks [cite: 499]"
Rl forget less,"de par sa KL divergence implicite, le RL permet de moins catastrophic forgetting. [cite: 499]Cela peut s'étudier en regardant la KL sur une tâche originale non fine-tunée. [cite: 500]Si j'ai bien compris c'est fait en Dr GRPO sans KL donc vraiment implicite [cite: 501]","because of its implicit KL divergence, RL allows for less catastrophic forgetting. [cite: 499]This can be studied by looking at the KL on an original non-fine-tuned task. [cite: 500]If I understood correctly, it is done in Dr GRPO without KL, so truly implicit [cite: 501]"
Jina reranker v3,"en gros l'idée est de mettre la query et les docs à rerank dans le même contexte, d'extraire les embeddings de chaque document par rapport à la query puis MLP pour réduction et faire cosine sur cette représentation quasi bidirectionnelle. [cite: 501]La loss porte sur le rank, sur la query au début de la séquence et à la fin, sur la diversité des embeddings de différents doc et sur la robustesse au rephrasing d'un même document [cite: 502]","basically the idea is to put the query and the docs to rerank in the same context, extract the embeddings of each document relative to the query then MLP for reduction and do cosine on this quasi-bidirectional representation. [cite: 501]The loss focuses on the rank, on the query at the beginning and end of the sequence, on the diversity of embeddings from different docs, and on robustness to rephrasing of the same document [cite: 502]"
Deepseek OCR,l'idée est de voir comment compresser l'image en gardant des perfs OCR très bonnes. [cite: 503]On peut compresser par 10 ou 15 les tokens images par rapport aux token text. [cite: 504]L'architecture est un ViT frozen qui fait les patch (tokenizer) un CLIP train qui fait l'embedding et un décoder Deepseek. [cite: 505]Rapide et performant. [cite: 506],"the idea is to see how to compress the image while maintaining very good OCR performance. [cite: 503]Image tokens can be compressed by 10 or 15 compared to text tokens. [cite: 504]The architecture is a frozen ViT that creates the patches (tokenizer), a trained CLIP that creates the embedding, and a Deepseek decoder. [cite: 505]Fast and performant. [cite: 506]"
Better with less,"objectif est l'extraction d'un vendeur dans un libellé de transaction complexe. [cite: 506]Étude sur fine tune vs petit modèle from scratch et encoder / décoder ou les 2. Le mieux et moins cher est un décoder only de 11M params avec un vocabulaire de 1000 token et BPE. [cite: 507]Le dataset fait environ 1 million de transactions avec du rule based, du strings matching et du human labelled. [cite: 508]","objective is the extraction of a seller in a complex transaction label. [cite: 506] Study on fine-tune vs small model from scratch and encoder/decoder or both. The best and cheapest is a decoder-only model of 11M params with a vocabulary of 1000 tokens and BPE. [cite: 507]The dataset is about 1 million transactions with rule-based, string matching, and human-labeled data. [cite: 508]"
Deepseek v3.2 (DSA),"DSA c'est en fait un branchement au MLA avec un ReLU sur q,k avec k qui est en fait un k avec une seule attention head en FP8 qui permet de sélectionner un top k avec cette attention à coût réduit pour ensuite faire la vraie attention avec que le top k. [cite: 509]Initialisation de ça avec une loss custom en continuous learning depuis v3.1 [cite: 510]","DSA is actually a branching to MLA with a ReLU on q,k where k is actually a k with a single FP8 attention head that allows selecting a top k with this low-cost attention to then do the real attention with only the top k. [cite: 509]Initialization of this with a custom loss in continuous learning since v3.1 [cite: 510]"
Linear attention,on remplace l'exp du softmax par juste le dot product ce qui fait qu'on a quelque chose de linéaire après quelques simplifications. [cite: 510]On a la somme continue de kvT qui est une récurrence. [cite: 511],we replace the exp of the softmax with just the dot product which makes it linear after some simplifications. [cite: 510]We have the continuous sum of kvT which is a recurrence. [cite: 511]
Deltanet attention,"Le problème de Linear attention est que dans S (qui est la somme continue de kvT) on stack tout. [cite: 512]Delta net permet de supprimer une partie des anciennes valeurs et ajouter le nouveau produit avec un ""learning rate"" c'est la delta rule. [cite: 513]Ça marche mieux que simple linear. [cite: 514]","The problem with Linear attention is that in S (which is the continuous sum of kvT) we stack everything. [cite: 512]Delta net allows removing part of the old values and adding the new product with a ""learning rate""; this is the delta rule. [cite: 513]It works better than simple linear. [cite: 514]"
Gated attention,juste attention avec linear et sigmoid sur le skip connection pour stabiliser les low ranks ? [cite: 514],just attention with linear and sigmoid on the skip connection to stabilize low ranks? [cite: 514]
Tml on policy distillation,off policy on essaye de matcher un teacher (SFT ou off policy distillation où on apprend via KL au student à matcher la distribution de logits du teacher). [cite: 515]RL on ne travaille pas la trajectoire que l'output donc pas d'indices (sauf process reward model). [cite: 516]Donc on policy distillation on sample des rollouts comme en RL et on compute les log prob avec un modèle plus fort qui va comme corriger le rollout et on fait une loss KL divergence (en tout cas l'advantage est la KL). [cite: 517]Ça peut permettre le continuous learning aussi et de retrouver des capacités perdues. [cite: 518],"off-policy we try to match a teacher (SFT or off-policy distillation where we use KL to teach the student to match the teacher's logit distribution). [cite: 515]In RL we don't work on the trajectory but on the output, so no intermediate indices (except for process reward model). [cite: 516]So in on-policy distillation we sample rollouts like in RL and compute the log prob with a stronger model that will correct the rollout, and we use a KL divergence loss (in any case the advantage is the KL). [cite: 517]This can also allow for continuous learning and for regaining lost capabilities. [cite: 518]"